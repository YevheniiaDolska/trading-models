import numpy as np
import pandas as pd
import tensorflow as tf
import os
from datetime import datetime, timedelta
from binance.client import Client
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.preprocessing import RobustScaler
from sklearn.utils.class_weight import compute_class_weight
import joblib
import logging
import pandas_ta as ta
import requests
from scipy.stats import zscore
from tensorflow.keras.metrics import Precision, Recall, AUC
from tensorflow.keras.regularizers import l2
import time
from binance.exceptions import BinanceAPIException
import zipfile
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor
import tensorflow.keras.backend as K
import xgboost as xgb
from tensorflow.keras.layers import Layer, GRU, Input, Concatenate
from sklearn.ensemble import VotingClassifier
import matplotlib.pyplot as plt
from tensorflow.keras.backend import clear_session
import glob
import requests
import zipfile
from io import BytesIO
from threading import Lock
from ta.trend import SMAIndicator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Input, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split, KFold
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import tensorflow.keras.backend as K
from sklearn.model_selection import StratifiedKFold
from sklearn.dummy import DummyClassifier
from scipy.stats import zscore  



# ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
def initialize_strategy():
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            logging.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU!")
            return tf.distribute.MirroredStrategy()
        except RuntimeError as e:
            logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU: {e}")
            return tf.distribute.get_strategy()
    else:
        logging.info("‚ùå GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
        return tf.distribute.get_strategy()

    

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def cleanup_training_files():
    """
    –£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª—ã —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    """
    files_to_delete = glob.glob("binance_data*.csv")  # –ò—â–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö
    for file_path in files_to_delete:
        try:
            os.remove(file_path)
            logging.info(f"üóë –£–¥–∞–ª—ë–Ω —Ñ–∞–π–ª: {file_path}")
        except Exception as e:
            logging.error(f"‚ö† –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {file_path}: {e}")
            
            

class Attention(Layer):
    """Attention-–º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫"""
    def __init__(self):
        super(Attention, self).__init__()

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1), initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(1,), initializer="zeros")
        super(Attention, self).build(input_shape)

    def call(self, x):
        e = K.tanh(K.dot(x, self.W) + self.b)
        a = K.softmax(e, axis=1)
        return x * a
    
class MarketClassifier:
    def __init__(self, model_path="market_condition_classifier.h5", scaler_path="scaler.pkl"):
        self.model_path = model_path
        self.scaler_path = scaler_path
        self.base_url = "https://data.binance.vision/data/spot/monthly/klines/{symbol}/1m/"

    def fetch_binance_data(self, symbol, interval, start_date, end_date):
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å Binance –±–µ–∑ API-–∫–ª—é—á–∞ (–∞—Ä—Ö–∏–≤ Binance) –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: timestamp, open, high, low, close, volume.
        """
        base_url_monthly = "https://data.binance.vision/data/spot/monthly/klines"
        logging.info(f"üì° –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å Binance –¥–ª—è {symbol} ({interval}) c {start_date} –ø–æ {end_date}...")

        start_date = pd.to_datetime(start_date)
        end_date = pd.to_datetime(end_date)
        all_data = []
        downloaded_files = set()
        download_lock = Lock()  # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è

        def download_and_process(date):
            year, month = date.year, date.month
            month_str = f"{month:02d}"
            file_name = f"{symbol}-{interval}-{year}-{month_str}.zip"
            file_url = f"{base_url_monthly}/{symbol}/{interval}/{file_name}"

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            with download_lock:
                if file_name in downloaded_files:
                    logging.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫ {file_name}, —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
                    return None

                logging.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è {file_url}...")
                response = requests.head(file_url, timeout=5)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_url}")
                    return None

                logging.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {file_url}...")
                response = requests.get(file_url, timeout=15)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_url}: –ö–æ–¥ {response.status_code}")
                    return None

                logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω {file_name}")
                downloaded_files.add(file_name)

            try:
                with zipfile.ZipFile(BytesIO(response.content)) as zip_file:
                    csv_file = file_name.replace('.zip', '.csv')
                    with zip_file.open(csv_file) as file:
                        df = pd.read_csv(
                            file, header=None, 
                            names=[
                                "timestamp", "open", "high", "low", "close", "volume",
                                "close_time", "quote_asset_volume", "number_of_trades",
                                "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"
                            ],
                            dtype={
                                "timestamp": "int64",
                                "open": "float32",
                                "high": "float32",
                                "low": "float32",
                                "close": "float32",
                                "volume": "float32",
                                "quote_asset_volume": "float32",
                                "number_of_trades": "int32",
                                "taker_buy_base_asset_volume": "float32",
                                "taker_buy_quote_asset_volume": "float32"
                            },
                            low_memory=False
                        )
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime
                        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
                        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
                        # –ü—Ä–∏–≤–æ–¥–∏–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∫ —Ç–∏–ø—É float, –Ω–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—è timestamp
                        df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].astype(float)
                        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º timestamp –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
                        df.set_index("timestamp", inplace=True)
                        return df
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {symbol} –∑–∞ {date.strftime('%Y-%m')}: {e}")
                return None

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ
        with ThreadPoolExecutor(max_workers=8) as executor:
            results = list(executor.map(download_and_process, pd.date_range(start=start_date, end=end_date, freq='MS')))

        # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        all_data = [df for df in results if df is not None]

        if not all_data:
            raise ValueError(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}.")

        df = pd.concat(all_data)
        logging.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {df.shape}")

        # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –∫–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–∞–∫ —Å—Ç–æ–ª–±–µ—Ü, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å
        if "timestamp" not in df.columns:
            df.reset_index(inplace=True)

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ timestamp –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
        df.set_index("timestamp", inplace=True)

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–æ 1-–º–∏–Ω—É—Ç–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        df = df.resample('1min').ffill()

        # –°–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞, —á—Ç–æ–±—ã timestamp —Å—Ç–∞–ª –æ–±—ã—á–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π
        df.reset_index(inplace=True)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        num_nans = df.isna().sum().sum()
        if num_nans > 0:
            if num_nans / len(df) > 0.05:  # –ï—Å–ª–∏ –±–æ–ª–µ–µ 5% –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—â–µ–Ω—ã
                logging.warning("‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö! –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–∏ —Å–≤–µ—á–∏.")
                df.dropna(inplace=True)
            else:
                df.fillna(method='ffill', inplace=True)

        logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        return df

        

    def add_indicators(self, data):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pandas_ta"""
        # –ë–∞–∑–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        data['atr'] = ta.atr(data['high'], data['low'], data['close'], length=14)
        data['adx'] = ta.adx(data['high'], data['low'], data['close'], length=14)['ADX_14']
        
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ MA –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–∏–ª—ã —Ç—Ä–µ–Ω–¥–∞
        for period in [10, 20, 50, 100]:
            data[f'sma_{period}'] = ta.sma(data['close'], length=period)
            data[f'ema_{period}'] = ta.ema(data['close'], length=period)
        
        # –ò–º–ø—É–ª—å—Å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        data['rsi'] = ta.rsi(data['close'], length=14)
        macd = ta.macd(data['close'], fast=12, slow=26, signal=9)
        data['macd'], data['macd_signal'], data['macd_hist'] = macd['MACD_12_26_9'], macd['MACDs_12_26_9'], macd['MACDh_12_26_9']
        data['willr'] = ta.willr(data['high'], data['low'], data['close'], length=14)
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        bb = ta.bbands(data['close'], length=20, std=2)
        data['bb_upper'], data['bb_middle'], data['bb_lower'] = bb['BBU_20_2.0'], bb['BBM_20_2.0'], bb['BBL_20_2.0']
        data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']
        
        # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        data['obv'] = ta.obv(data['close'], data['volume'])
        data['volume_sma'] = data['volume'].rolling(window=20).mean()
        data['volume_ratio'] = data['volume'] / data['volume_sma']
        data['volume_std'] = data['volume'].rolling(window=20).std()
        
        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        data['trend_strength'] = abs(data['close'].pct_change().rolling(window=20).sum())
        data['price_momentum'] = data['close'].diff(periods=10) / data['close'].shift(10)
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —É—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
        data['support_level'] = data['low'].rolling(window=20).min()
        data['resistance_level'] = data['high'].rolling(window=20).max()
        
        return data

    
    
    def validate_predictions(self, data, prediction, window=5):
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–ø–µ—Ä–∏–æ–¥–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–≤–µ—á–µ–π –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª–∞
        recent_data = data.tail(window)
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–Ω–¥–∞
        price_direction = recent_data['close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)
        trend_consistency = abs(price_direction.sum()) / window
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä–µ–º–Ω–æ–≥–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        volume_trend = recent_data['volume'] > recent_data['volume_sma']
        volume_confirmation = volume_trend.mean()
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø—É–ª—å—Å–∞
        momentum_confirmation = (
            (recent_data['rsi'] > 60).all() or  # –°–∏–ª—å–Ω—ã–π –±—ã—á–∏–π –∏–º–ø—É–ª—å—Å
            (recent_data['rsi'] < 40).all() or  # –°–∏–ª—å–Ω—ã–π –º–µ–¥–≤–µ–∂–∏–π –∏–º–ø—É–ª—å—Å
            (recent_data['rsi'].between(45, 55)).all()  # –°—Ç–∞–±–∏–ª—å–Ω—ã–π —Ñ–ª—ç—Ç
        )
        
        # 4. –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã
        mtf_confirmation = (
            recent_data['adx'].mean() > 25 and  # –°–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥
            abs(recent_data['macd_hist']).mean() > recent_data['macd_hist'].std()  # –°–∏–ª—å–Ω—ã–π MACD
        )
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å–∫–æ—Ä–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
        confidence_score = (
            0.3 * trend_consistency +
            0.3 * volume_confirmation +
            0.2 * momentum_confirmation +
            0.2 * mtf_confirmation
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
        if prediction == 'bullish':
            prediction_valid = (
                trend_consistency > 0.6 and
                volume_confirmation > 0.5 and
                recent_data['rsi'].mean() > 55
            )
        elif prediction == 'bearish':
            prediction_valid = (
                trend_consistency > 0.6 and
                volume_confirmation > 0.5 and
                recent_data['rsi'].mean() < 45
            )
        else:  # flat
            prediction_valid = (
                trend_consistency < 0.4 and
                0.3 < volume_confirmation < 0.7 and
                40 < recent_data['rsi'].mean() < 60
            )
        
        return {
            'prediction': prediction,
            'is_valid': prediction_valid,
            'confidence': confidence_score,
            'confirmations': {
                'trend_consistency': trend_consistency,
                'volume_confirmation': volume_confirmation,
                'momentum_confirmation': momentum_confirmation,
                'mtf_confirmation': mtf_confirmation
            }
        }
    
    
    def classify_market_conditions(self, data, window=20):
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä—ã–Ω–∫–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è–º–∏
        """
        if len(data) < window:
            logging.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {len(data)} < {window}")
            return 'flat'  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º flat –≤–º–µ—Å—Ç–æ uncertain –∫–∞–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            
        if not hasattr(self, 'previous_market_type'):
            self.previous_market_type = 'flat'  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–µ–º flat

        # –ë–∞–∑–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        adx = data['adx'].iloc[-1]
        rsi = data['rsi'].iloc[-1]
        macd_hist = data['macd_hist'].iloc[-1]
        willr = data['willr'].iloc[-1]
        volume_ratio = data['volume_ratio'].iloc[-1]
        price = data['close'].iloc[-1]
        support = data['support_level'].iloc[-1]
        resistance = data['resistance_level'].iloc[-1]
        
        # –†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ —É—Ä–æ–≤–Ω–µ–π –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
        distance_to_support = ((price - support) / price) * 100
        distance_to_resistance = ((resistance - price) / price) * 100
        
        logging.info(f"""
        –¢–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤:
        ADX: {adx:.2f}
        RSI: {rsi:.2f}
        MACD Histogram: {macd_hist:.2f}
        Williams %R: {willr:.2f}
        Volume Ratio: {volume_ratio:.2f}
        –¶–µ–Ω–∞: {price:.2f}
        –ü–æ–¥–¥–µ—Ä–∂–∫–∞: {support:.2f} (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance_to_support:.2f}%)
        –°–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ: {resistance:.2f} (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance_to_resistance:.2f}%)
        """)

        # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞ —á–µ—Ä–µ–∑ MA —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        ma_trends = []
        for period in [10, 20, 50]:
            is_above = price > data[f'sma_{period}'].iloc[-1]
            ma_trends.append(is_above)
            logging.info(f"–¶–µ–Ω–∞ –≤—ã—à–µ SMA{period}: {is_above}")
        trend_confirmation = sum(ma_trends)
        logging.info(f"–û–±—â–µ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞: {trend_confirmation}/3")

        # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–≤–µ—á–∞—Ö
        recent_data = data.tail(window)
        price_direction = recent_data['close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)
        trend_consistency = abs(price_direction.sum()) / window
        volume_confirmation = (recent_data['volume'] > recent_data['volume_sma']).mean()
        momentum_strength = abs(recent_data['rsi'] - 50).mean() / 50

        logging.info(f"""
        –ú–µ—Ç—Ä–∏–∫–∏ —Ç—Ä–µ–Ω–¥–∞:
        –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–¥–∞: {trend_consistency:.2f}
        –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–±—ä—ë–º–æ–º: {volume_confirmation:.2f}
        –°–∏–ª–∞ –∏–º–ø—É–ª—å—Å–∞: {momentum_strength:.2f}
        """)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ä—ã–Ω–∫–∞ —Å –±–æ–ª–µ–µ –º—è–≥–∫–∏–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏
        market_type = 'uncertain'
        confidence_score = 0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±—ã—á—å–µ–≥–æ —Ä—ã–Ω–∫–∞
        if (adx > 20 and rsi > 45 and volume_ratio > 1.0 and
            trend_confirmation >= 1 and macd_hist > 0 and
            distance_to_resistance > 0.5):  # –ï—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è —Ä–æ—Å—Ç–∞
            
            bullish_confidence = (
                0.25 * (trend_consistency if price_direction.sum() > 0 else 0) +
                0.25 * volume_confirmation +
                0.25 * (momentum_strength if rsi > 45 else 0) +
                0.25 * (distance_to_resistance / 5)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
            )
            
            logging.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –±—ã—á—å–µ–≥–æ —Ä—ã–Ω–∫–∞. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {bullish_confidence:.2f}")
            
            if bullish_confidence > 0.5:
                market_type = 'bullish'
                confidence_score = bullish_confidence

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞
        elif (adx > 20 and rsi < 55 and volume_ratio > 1.0 and
              trend_confirmation <= 2 and macd_hist < 0 and
              distance_to_support > 0.5):  # –ï—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –ø–∞–¥–µ–Ω–∏—è
            
            bearish_confidence = (
                0.25 * (trend_consistency if price_direction.sum() < 0 else 0) +
                0.25 * volume_confirmation +
                0.25 * (momentum_strength if rsi < 55 else 0) +
                0.25 * (distance_to_support / 5)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –ø–æ–¥–¥–µ—Ä–∂–∫–∏
            )
            
            logging.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {bearish_confidence:.2f}")
            
            if bearish_confidence > 0.5:
                market_type = 'bearish'
                confidence_score = bearish_confidence

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–ª—ç—Ç–∞
        elif (adx < 25 and 35 < rsi < 65 and
              abs(macd_hist) < 0.2 * data['macd_hist'].std() and
              0.7 < volume_ratio < 1.3 and
              support < price < resistance and
              max(distance_to_support, distance_to_resistance) < 2):  # –¶–µ–Ω–∞ –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏
            
            flat_confidence = (
                0.25 * (1 - trend_consistency) +
                0.25 * (1 - abs(volume_ratio - 1)) +
                0.25 * (1 - momentum_strength) +
                0.25 * (1 - max(distance_to_support, distance_to_resistance) / 2)
            )
            
            logging.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–ª—ç—Ç–∞. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {flat_confidence:.2f}")
            
            if flat_confidence > 0.5:
                market_type = 'flat'
                confidence_score = flat_confidence

        # –ï—Å–ª–∏ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
        if market_type == 'uncertain':
            # –£—á–∏—Ç—ã–≤–∞–µ–º —É—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è –≤ —É–ø—Ä–æ—â—ë–Ω–Ω–æ–π –ª–æ–≥–∏–∫–µ
            if rsi > 60 and distance_to_resistance > 0.5:
                market_type = 'bullish'
            elif rsi < 40 and distance_to_support > 0.5:
                market_type = 'bearish'
            else:
                market_type = 'flat'
            
            logging.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é: {market_type} (RSI: {rsi:.2f}, " +
                        f"–†–∞—Å—Å—Ç. –¥–æ —Å–æ–ø—Ä.: {distance_to_resistance:.2f}%, " +
                        f"–†–∞—Å—Å—Ç. –¥–æ –ø–æ–¥–¥.: {distance_to_support:.2f}%)")
        
        self.previous_market_type = market_type
        return market_type
    

    def remove_outliers(self, data, z_threshold=3):
        """
        –£–¥–∞–ª—è–µ—Ç –≤—ã–±—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–∞ Z-score.
        """
        z_scores = zscore(data[['open', 'high', 'low', 'close', 'volume']])
        mask = (np.abs(z_scores) < z_threshold).all(axis=1)
        filtered_data = data[mask]
        removed_count = len(data) - len(filtered_data)
        logging.info(f"–£–¥–∞–ª–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {removed_count}")
        return filtered_data

    def fetch_market_events(self, api_key, start_date, end_date):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è —á–µ—Ä–µ–∑ API.
        """
        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                "q": "market crash OR economic crisis OR volatility",
                "from": start_date.strftime("%Y-%m-%d"),
                "to": end_date.strftime("%Y-%m-%d"),
                "sortBy": "relevancy",
                "language": "en",
                "apiKey": api_key,
            }
            response = requests.get(url, params=params)
            if response.status_code == 200:
                articles = response.json().get("articles", [])
                events = []
                for article in articles:
                    published_date = datetime.strptime(article['publishedAt'], "%Y-%m-%dT%H:%M:%SZ")
                    events.append({
                        "start": published_date - timedelta(days=1),
                        "end": published_date + timedelta(days=1),
                        "event": article['title']
                    })
                return events
            else:
                logging.error(f"–û—à–∏–±–∫–∞ API: {response.status_code} {response.text}")
                return []
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π: {e}")
            return []

    def flag_market_events(self, data, events):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ñ–ª–∞–≥–∏ –¥–ª—è —Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π.
        """
        data['market_event'] = 'None'
        for event in events:
            mask = (data.index >= event['start']) & (data.index <= event['end'])
            data.loc[mask, 'market_event'] = event['event']
        logging.info(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–ª–∞–≥–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –¥–æ–±–∞–≤–ª–µ–Ω—ã.")
        return data

    def fetch_and_label_all(self, symbols, start_date, end_date, save_path="labeled_data"):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ —Ä–∞–∑–º–µ—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API.
        """
        os.makedirs(save_path, exist_ok=True)
        all_data = []

        for symbol in symbols:
            try:
                logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
                df = self.fetch_binance_data(symbol, "1m", start_date, end_date)  # ‚úÖ –£–±—Ä–∞–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API
                df = self.add_indicators(df)
                df['market_type'] = self.classify_market_conditions(df)
                df['symbol'] = symbol
                file_path = os.path.join(save_path, f"{symbol}_data.csv")
                df.to_csv(file_path)
                logging.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {file_path}")
                all_data.append(df)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {symbol}: {e}")

        if not all_data:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞.")
        
        return pd.concat(all_data, ignore_index=True)


    
    def prepare_training_data(self, data_path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        """
        logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ {data_path}")
        try:
            data = pd.read_csv(data_path, index_col=0)
        except FileNotFoundError:
            logging.error(f"–§–∞–π–ª {data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            raise

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
        required_columns = [
            'open', 'high', 'low', 'close', 'volume',
            'atr', 'adx', 'rsi', 'macd', 'macd_signal', 'macd_hist',
            'willr', 'bb_width', 'volume_ratio', 'trend_strength',
            'market_type'
        ]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π —Å—Ç–æ–ª–±–µ—Ü: {col}")

        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        data.dropna(inplace=True)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π market_type –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logging.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è market_type –¥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {data['market_type'].unique()}")

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ (—Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –≤—ã–∑–æ–≤!)
        logging.info("–£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö...")
        data = self.remove_outliers(data)
        logging.info(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤: {data.shape}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ —Ä—ã–Ω–∫–∞ –≤ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        label_mapping = {'bullish': 0, 'bearish': 1, 'flat': 2}
        data['market_type'] = data['market_type'].map(label_mapping)

        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NaN –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        if data['market_type'].isna().any():
            bad_values = data[data['market_type'].isna()]['market_type'].unique()
            logging.error(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ market_type! –ò—Å—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {bad_values}")
            data.dropna(subset=['market_type'], inplace=True)

        features = [col for col in data.columns if col not in ['market_type', 'symbol', 'timestamp']]
        X = data[features].values
        y = data['market_type'].values.astype(int)

        logging.info(f"–§–æ—Ä–º–∞ X: {X.shape}")
        logging.info(f"–§–æ—Ä–º–∞ y: {y.shape}")

        return X, y



    def balance_classes(self, y):
        # –ü—Ä–∏–≤–æ–¥–∏–º y –∫ numpy-–º–∞—Å—Å–∏–≤—É, –µ—Å–ª–∏ —ç—Ç–æ –µ—â—ë –Ω–µ —Å–¥–µ–ª–∞–Ω–æ
        y = np.array(y)
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ y
        present_classes = np.unique(y)
        computed_weights = compute_class_weight(
            class_weight='balanced',
            classes=present_classes,
            y=y
        )
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ç–µ—Ö –∫–ª–∞—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å
        class_weights = {int(cls): weight for cls, weight in zip(present_classes, computed_weights)}
        
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ —Ç—Ä–∏ –∫–ª–∞—Å—Å–∞: 0, 1 –∏ 2
        for cls in [0, 1, 2]:
            if cls not in class_weights:
                # –ï—Å–ª–∏ –∫–ª–∞—Å—Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ y, —Ç–æ –µ–≥–æ –≤–µ—Å –≤–∑—è—Ç—å —Ä–∞–≤–Ω—ã–º 1.0
                # (–≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —ç—Ç–æ—Ç —Å–ª—É—á–∞–π –≤–æ–∑–Ω–∏–∫–∞—Ç—å –Ω–µ –¥–æ–ª–∂–µ–Ω)
                class_weights[cls] = 1.0
        return class_weights





    def build_lstm_gru_model(self, input_shape):
        """–°–æ–∑–¥–∞—ë—Ç –º–æ—â–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å LSTM + GRU —Å Attention"""
        inputs = Input(shape=input_shape)

        # LSTM –±–ª–æ–∫
        lstm_out = LSTM(256, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)
        lstm_out = Dropout(0.3)(lstm_out)
        lstm_out = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(lstm_out)
        lstm_out = Dropout(0.3)(lstm_out)
        lstm_out = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(lstm_out)

        # GRU –±–ª–æ–∫
        gru_out = GRU(256, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)
        gru_out = Dropout(0.3)(gru_out)
        gru_out = GRU(128, return_sequences=True, kernel_regularizer=l2(0.01))(gru_out)
        gru_out = Dropout(0.3)(gru_out)
        gru_out = GRU(64, return_sequences=True, kernel_regularizer=l2(0.01))(gru_out)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—ã—Ö–æ–¥—ã LSTM –∏ GRU
        combined = Concatenate()([lstm_out, gru_out])

        # Attention-–º–µ—Ö–∞–Ω–∏–∑–º
        attention = Attention()(combined)

        # –§–∏–Ω–∞–ª—å–Ω—ã–µ Dense-—Å–ª–æ–∏
        x = LSTM(64, return_sequences=False)(attention)
        x = Dropout(0.3)(x)
        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)
        x = Dropout(0.3)(x)
        x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)
        outputs = Dense(3, activation='softmax')(x)  # 3 –∫–ª–∞—Å—Å–∞: bullish, bearish, flat

        model = tf.keras.models.Model(inputs, outputs)
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        return model


    def train_xgboost(self, X_train, y_train):
        """–û–±—É—á–∞–µ—Ç XGBoost –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö LSTM + GRU, –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DummyClassifier, –µ—Å–ª–∏ –≤ y_train —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å."""
        unique_classes = np.unique(y_train)
        if len(unique_classes) < 2:
            logging.warning("–í –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ XGB –æ–±–Ω–∞—Ä—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å. –ò—Å–ø–æ–ª—å–∑—É–µ–º DummyClassifier.")
            dummy = DummyClassifier(strategy='constant', constant=unique_classes[0])
            dummy.fit(X_train, y_train)
            return dummy
        else:
            booster = xgb.XGBClassifier(
                objective='multi:softmax',
                num_class=3,
                learning_rate=0.1,
                n_estimators=10,
                max_depth=3,
                subsample=0.8,
                colsample_bytree=0.8,
                verbosity=1
            )
            booster.fit(X_train, y_train)
            return booster


    def build_ensemble(X_train, y_train):
        """–§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å: LSTM + GRU + XGBoost"""
        lstm_gru_model = build_lstm_gru_model((X_train.shape[1], X_train.shape[2]))

        # –û–±—É—á–∞–µ–º LSTM-GRU
        lstm_gru_model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1)#epochs=100

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è –ø–µ—Ä–µ–¥ softmax
        feature_extractor = tf.keras.models.Model(
            inputs=lstm_gru_model.input, outputs=lstm_gru_model.layers[-3].output
        )
        X_features = feature_extractor.predict(X_train)

        # –û–±—É—á–∞–µ–º XGBoost –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö
        xgb_model = self.train_xgboost(X_features, y_train)

        # –ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ VotingClassifier
        ensemble = VotingClassifier(
            estimators=[
                ('lstm_gru', lstm_gru_model),
                ('xgb', xgb_model)
            ],
            voting='soft'
        )
        return ensemble


    def train_market_condition_classifier(self, data_path, model_path='market_condition_classifier.h5',
                                          scaler_path='scaler.pkl', checkpoint_path='market_condition_checkpoint.h5'):
        """
        –û–±—É—á–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–Ω—Å–∞–º–±–ª–µ–≤—É—é –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
        –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LSTM + GRU + Attention + XGBoost –∏ —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 85% —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º.
        """
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = self.prepare_training_data(data_path)

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        scaler = RobustScaler()  # ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ (—Ç–µ–ø–µ—Ä—å RobustScaler –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è)
        if os.path.exists(scaler_path):
            logging.info(f"–ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ –∏–∑ {scaler_path}.")
            scaler = joblib.load(scaler_path)
        else:
            logging.info("–°–æ–∑–¥–∞—ë—Ç—Å—è –Ω–æ–≤—ã–π –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫.")
            scaler.fit(X)
            joblib.dump(scaler, scaler_path)
            logging.info(f"–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {scaler_path}.")

        X_scaled = scaler.transform(X)

        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏
        skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)
        fold_scores = []
        f1_scores = []

        for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y)):
            X_train_fold, X_val_fold = X_scaled[train_idx], X_scaled[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM
            X_train_fold = np.expand_dims(X_train_fold, axis=1)
            X_val_fold = np.expand_dims(X_val_fold, axis=1)

            logging.info(f"–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ñ–æ–ª–¥–µ {fold + 1}")
            logging.info(f"Shape X_train_fold: {X_train_fold.shape}")
            logging.info(f"Shape y_train_fold: {y_train_fold.shape}")

            # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ñ–æ–ª–¥–∞
            with strategy.scope():
                model_fold = self.build_lstm_gru_model(input_shape=(X_train_fold.shape[1], X_train_fold.shape[2]))  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å GRU + Attention

                model_fold.fit(
                    X_train_fold, y_train_fold,
                    epochs=50,#50
                    batch_size=64,
                    validation_data=(X_val_fold, y_val_fold),
                    verbose=1
                )

            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            y_val_pred = model_fold.predict(X_val_fold)
            y_val_pred_classes = np.argmax(y_val_pred, axis=1)

            val_acc = accuracy_score(y_val_fold, y_val_pred_classes)
            val_f1 = f1_score(y_val_fold, y_val_pred_classes, average='weighted')

            fold_scores.append(val_acc)
            f1_scores.append(val_f1)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏
        avg_accuracy = np.mean(fold_scores)
        avg_f1_score = np.mean(f1_scores)

        logging.info(f"–°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {avg_accuracy:.4f}")
        logging.info(f"–°—Ä–µ–¥–Ω–∏–π F1-score –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {avg_f1_score:.4f}")

        if avg_accuracy < 0.85 or avg_f1_score < 0.80:
            logging.warning("–ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∏–∂–µ 85% —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–ª–∏ 80% F1-score. –î–æ—Ä–∞–±–æ—Ç–∞–π—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É.")
            return None  # –ï—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∏–∑–∫–æ–µ, –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ–π –≤—ã–±–æ—Ä–∫–µ
        X_scaled = np.expand_dims(X_scaled, axis=1)  # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –æ—Å—å –¥–ª—è LSTM

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
        class_weights = self.balance_classes(y_train)

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        with strategy.scope():
            final_model = self.build_lstm_gru_model(input_shape=(X_train.shape[1], X_train.shape[2]))  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º LSTM + GRU + Attention

            callbacks = [
                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
                ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', verbose=1),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)
            ]

            history = final_model.fit(
                X_train, y_train,
                validation_data=(X_test, y_test),
                epochs=200,#200
                batch_size=64,
                class_weight=class_weights,
                callbacks=callbacks
            )

            # **–û–±—É—á–∞–µ–º XGBoost –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö LSTM + GRU**
            feature_extractor = tf.keras.models.Model(
                inputs=final_model.input, outputs=final_model.layers[-3].output  # ‚úÖ –ë–µ—Ä–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–µ—Ä–µ–¥ softmax
            )
            X_train_features = feature_extractor.predict(X_train)
            X_test_features = feature_extractor.predict(X_test)

            xgb_model = self.train_xgboost(X_train_features, y_train)  # ‚úÖ –û–±—É—á–∞–µ–º XGBoost

            # **–û—Ü–µ–Ω–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏**
            y_pred_lstm_gru = final_model.predict(X_test)
            y_pred_xgb = xgb_model.predict(X_test_features)

            # **–ê–Ω—Å–∞–º–±–ª—å –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ–º**
            y_pred_classes = np.argmax(y_pred_lstm_gru, axis=1) * 0.5 + y_pred_xgb * 0.5
            y_pred_classes = np.round(y_pred_classes).astype(int)

            accuracy = accuracy_score(y_test, y_pred_classes)
            precision = precision_score(y_test, y_pred_classes, average='weighted')
            recall = recall_score(y_test, y_pred_classes, average='weighted')
            f1 = f1_score(y_test, y_pred_classes, average='weighted')

            logging.info(f"""
                –ú–µ—Ç—Ä–∏–∫–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏:
                Accuracy: {accuracy:.4f}
                Precision: {precision:.4f}
                Recall: {recall:.4f}
                F1-Score: {f1:.4f}
            """)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ
            if f1 >= 0.80:
                final_model.save(model_path)
                joblib.dump(xgb_model, "xgb_model.pkl")  # ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º XGBoost –æ—Ç–¥–µ–ª—å–Ω–æ
                logging.info(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å LSTM-GRU —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
                logging.info(f"XGBoost-–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ xgb_model.pkl")

                # **–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è**
                plt.figure(figsize=(10, 6))
                plt.plot(history.history['loss'], label='Train Loss')
                plt.plot(history.history['val_loss'], label='Validation Loss', linestyle='--')
                plt.legend()
                plt.title('Train vs Validation Loss')
                plt.xlabel('Epochs')
                plt.ylabel('Loss')
                plt.grid(True)
                plt.show()

                return final_model
            else:
                logging.warning("–§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–æ–≤–æ–≥–æ (80% F1-score). –ú–æ–¥–µ–ª—å –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
                return None



if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (TPU –∏–ª–∏ CPU/GPU)
    strategy = initialize_strategy()
    
    
    symbols = ['BTCUSDC', 'ETHUSDC', 'BNBUSDC','XRPUSDC', 'ADAUSDC', 'SOLUSDC', 'DOTUSDC', 'LINKUSDC', 'TONUSDC', 'NEARUSDC']
    
    start_date = datetime(2017, 1, 1)
    end_date = datetime(2024, 9, 31)
    
    data_path = "labeled_market_data.csv"  # –ü—É—Ç—å –∫ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
    model_path = "market_condition_classifier.h5"  # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    scaler_path = "scaler.pkl"  # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫–∞

    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    classifier = MarketClassifier()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        logging.info("–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.")
        labeled_data = classifier.fetch_and_label_all(
            symbols=symbols,
            start_date=start_date,
            end_date=end_date,
            save_path="labeled_data"
        )
        labeled_data.to_csv(data_path, index=True)
        logging.info(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {data_path}.")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        exit(1)

    # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    try:
        logging.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.")
        classifier.train_market_condition_classifier(
            data_path=data_path,
            model_path=model_path,
            scaler_path=scaler_path
        )
        logging.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ.")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è: {e}")
        exit(1)

