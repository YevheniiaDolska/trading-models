import pandas as pd
import numpy as np
import tensorflow as tf
import time
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Add, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau
from tensorflow.keras import backend as K
from ta.trend import MACD, SMAIndicator, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator
from binance.client import Client
from datetime import datetime, timedelta
import logging
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from sklearn.cluster import KMeans
from imblearn.combine import SMOTETomek
from ta.trend import SMAIndicator, MACD, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator, ChaikinMoneyFlowIndicator
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
import sys
from tensorflow.keras.backend import clear_session
import glob
import requests
import zipfile
from io import BytesIO
from threading import Lock
from ta.trend import SMAIndicator
import matplotlib.pyplot as plt
from xgboost import XGBRegressor
from tensorflow.keras.models import Model
from xgboost import XGBClassifier
from sklearn.metrics import f1_score
import joblib
from filterpy.kalman import KalmanFilter



def initialize_strategy():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç TPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è CPU/GPU.
    """
    try:
        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
        tf.config.experimental_connect_to_cluster(tpu)
        tf.tpu.experimental.initialize_tpu_system(tpu)
        strategy = tf.distribute.TPUStrategy(tpu)
    except ValueError:
        print('Not connected to a TPU runtime. Using default strategy.')
        strategy = tf.distribute.get_strategy()  # Fallback to CPU/GPU strategy
    print('Running with strategy:', strategy)
    return strategy


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
nn_model_filename = os.path.join(os.getcwd(),'bearish_nn_model.h5')
log_file = 'training_log_bearish_nn.txt'

network_name = "bearish_neural_network"  # –ò–º—è –º–æ–¥–µ–ª–∏
checkpoint_path_regular = f"checkpoints/{network_name}_checkpoint_epoch_{{epoch:02d}}.h5"
checkpoint_path_best = f"checkpoints/{network_name}_best_model.h5"

def save_logs_to_file(log_message):
    with open(log_file, 'a') as log_f:
        log_f.write(f"{datetime.now()}: {log_message}\n")
        
        

def cleanup_training_files():
    """
    –£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª—ã —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    """
    files_to_delete = glob.glob("binance_data*.csv")  # –ò—â–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö
    for file_path in files_to_delete:
        try:
            os.remove(file_path)
            logging.info(f"üóë –£–¥–∞–ª—ë–Ω —Ñ–∞–π–ª: {file_path}")
        except Exception as e:
            logging.error(f"‚ö† –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {file_path}: {e}")
            
        
        
def calculate_cross_coin_features(data_dict):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä.
    
    Args:
        data_dict (dict): –°–ª–æ–≤–∞—Ä—å DataFrame'–æ–≤ –ø–æ –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç–µ
    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å DataFrame'–æ–≤ —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    btc_data = data_dict['BTCUSDC']
    
    for symbol, df in data_dict.items():
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å BTC
        df['btc_corr'] = df['close'].rolling(30).corr(btc_data['close'])
        
        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞ –∫ BTC
        df['rel_strength_btc'] = (df['close'].pct_change() - 
                                btc_data['close'].pct_change())
        
        # –ë–µ—Ç–∞ –∫ BTC
        df['beta_btc'] = (df['close'].pct_change().rolling(30).cov(
            btc_data['close'].pct_change()) / 
            btc_data['close'].pct_change().rolling(30).var())
        
        # –û–ø–µ—Ä–µ–∂–µ–Ω–∏–µ/—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞ BTC
        df['lead_lag_btc'] = df['close'].pct_change().shift(-1).rolling(10).corr(
            btc_data['close'].pct_change())
            
        data_dict[symbol] = df
        
    return data_dict

def detect_anomalies(data):
    """
    –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Å–≤–µ—á–∏.
    """
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º z-score –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    data['volume_zscore'] = ((data['volume'] - data['volume'].rolling(50).mean()) / 
                            data['volume'].rolling(50).std())
    data['price_zscore'] = ((data['close'] - data['close'].rolling(50).mean()) / 
                           data['close'].rolling(50).std())
    
    # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –ø–∞–¥–µ–Ω–∏–π
    data['is_anomaly'] = ((abs(data['volume_zscore']) > 3) & (data['close'] < data['close'].shift(1)) | 
                         (abs(data['price_zscore']) > 3))
    return data

def validate_volume_confirmation(data):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π –æ–±—ä–µ–º–æ–º.
    """
    # –û–±—ä–µ–º–Ω–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞
    data['volume_trend_conf'] = np.where(
        (data['close'] > data['close'].shift(1)) & 
        (data['volume'] > data['volume'].rolling(20).mean()),
        1,
        np.where(
            (data['close'] < data['close'].shift(1)) & 
            (data['volume'] > data['volume'].rolling(20).mean()),
            -1,
            0
        )
    )
    
    # –°–∏–ª–∞ –æ–±—ä–µ–º–Ω–æ–≥–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
    data['volume_strength'] = (data['volume'] / 
                             data['volume'].rolling(20).mean() * 
                             data['volume_trend_conf'])
    
    # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ–±—ä–µ–º–∞
    data['volume_accumulation'] = data['volume_trend_conf'].rolling(5).sum()
    
    return data

def remove_noise(data):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞ —Å –∫–æ–º–±–∏–Ω–∞—Ü–∏–µ–π —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞ –∏ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è.
    """
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö
    data = data.copy()
    
    # Kalman filter –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è —Ü–µ–Ω—ã
    kf = KalmanFilter(dim_x=2, dim_z=1)
    kf.x = np.array([[data['close'].iloc[0]], [0.]])
    kf.F = np.array([[1., 1.], [0., 1.]])
    kf.H = np.array([[1., 0.]])
    kf.P *= 10
    kf.R = 5
    kf.Q = np.array([[0.1, 0.1], [0.1, 0.1]])
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ö–∞–ª–º–∞–Ω–∞
    smoothed_prices = []
    for price in data['close']:
        kf.predict()
        kf.update(price)
        smoothed_prices.append(float(kf.x[0]))
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–ª—å—Ç—Ä –ö–∞–ª–º–∞–Ω–∞ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
    data['smoothed_close'] = smoothed_prices
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ EMA –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
    ema_smooth = data['close'].ewm(span=10, min_periods=1, adjust=False).mean()
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
    rolling_std = data['close'].rolling(window=20).std()
    rolling_mean = data['close'].rolling(window=20).mean()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã (z-score > 3)
    data['is_anomaly'] = abs(data['close'] - rolling_mean) > (3 * rolling_std)
    data['is_anomaly'] = data['is_anomaly'].astype(int)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ 0 –∏ 1
    
    # –í—ã—á–∏—Å–ª—è–µ–º "—á–∏—Å—Ç—ã–µ" –¥–≤–∏–∂–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –∞–Ω–æ–º–∞–ª–∏–π
    data['clean_returns'] = data['smoothed_close'].pct_change() * (1 - data['is_anomaly'])
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
    data = data.ffill().bfill()
    
    return data


def preprocess_market_data(data_dict):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π.
    """
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    data_dict = calculate_cross_coin_features(data_dict)
    
    for symbol, df in data_dict.items():
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
        df = detect_anomalies(df)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–º
        df = validate_volume_confirmation(df)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —à—É–º
        df = remove_noise(df)
        
        data_dict[symbol] = df
    
    return data_dict
    

# –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—é —É–±—ã—Ç–∫–æ–≤
def custom_profit_loss(y_true, y_pred):
    y_true = tf.cast(y_true, dtype=tf.float32)
    
    # –û—Å–Ω–æ–≤–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞
    diff = y_pred - y_true
    
    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–π —Ñ–∞–∫—Ç–æ—Ä –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –æ—à–∏–±–æ–∫ (–æ—Å—Ç–∞–≤–ª—è–µ–º —ç—Ç—É –≤–∞–∂–Ω—É—é —á–∞—Å—Ç—å)
    log_factor = tf.math.log1p(tf.abs(diff) + 1e-7)
    
    # –®—Ç—Ä–∞—Ñ—ã –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞ –Ω–∞ –º–∏–Ω—É—Ç–Ω–æ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–µ
    false_long_penalty = 2.5  # –°–∏–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ –∑–∞ –ª–æ–∂–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –ø–æ–∫—É–ø–∫–∏
    false_short_penalty = 1.5  # –£–º–µ—Ä–µ–Ω–Ω—ã–π —à—Ç—Ä–∞—Ñ –∑–∞ –ª–æ–∂–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –ø—Ä–æ–¥–∞–∂–∏
    missed_drop_penalty = 2.0  # –®—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ–ø—É—Å–∫ —Å–∏–ª—å–Ω–æ–≥–æ –ø–∞–¥–µ–Ω–∏—è
    
    # –†–∞—Å—á–µ—Ç –ø–æ—Ç–µ—Ä—å —Å —É—á–µ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞
    loss = tf.where(
        tf.logical_and(y_true == 0, y_pred > 0.5),  # –õ–æ–∂–Ω—ã–π —Å–∏–≥–Ω–∞–ª –Ω–∞ –ø–æ–∫—É–ø–∫—É
        false_long_penalty * tf.abs(diff) * log_factor,  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ log_factor
        tf.where(
            tf.logical_and(y_true == 2, y_pred < 0.5),  # –ü—Ä–æ–ø—É—Å–∫ —Å–∏–ª—å–Ω–æ–≥–æ –ø–∞–¥–µ–Ω–∏—è
            missed_drop_penalty * tf.abs(diff) * log_factor,  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ log_factor
            tf.where(
                tf.logical_and(y_true == 0, y_pred < 0.2),  # –õ–æ–∂–Ω—ã–π —Å–∏–≥–Ω–∞–ª –Ω–∞ –ø—Ä–æ–¥–∞–∂—É
                false_short_penalty * tf.abs(diff) * log_factor,  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ log_factor
                tf.abs(diff) * log_factor  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ log_factor
            )
        )
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    uncertainty_penalty = tf.where(
        tf.logical_and(y_pred > 0.3, y_pred < 0.7),
        0.5 * tf.abs(diff) * log_factor,  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ log_factor
        0.0
    )
    
    # –î–æ–±–∞–≤–∏—Ç—å —à—Ç—Ä–∞—Ñ –∑–∞ –∑–∞–¥–µ—Ä–∂–∫—É —Ä–µ–∞–∫—Ü–∏–∏
    time_penalty = 0.1 * tf.abs(diff) * tf.cast(tf.range(tf.shape(diff)[0]), tf.float32) / tf.cast(tf.shape(diff)[0], tf.float32)
    
    # –î–æ–±–∞–≤–∏—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è —É—á–µ—Ç–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –∏–∑–¥–µ—Ä–∂–µ–∫
    transaction_cost = 0.001 * tf.reduce_sum(tf.abs(y_pred[1:] - y_pred[:-1]))
    
    total_loss = tf.reduce_mean(loss + uncertainty_penalty)
    return total_loss

# Attention Layer
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], input_shape[-1]), initializer='glorot_uniform', trainable=True)
        self.b = self.add_weight(name='att_bias', shape=(input_shape[-1],), initializer='zeros', trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        e = K.tanh(K.dot(inputs, self.W) + self.b)
        a = K.softmax(e, axis=1)
        output = inputs * a
        return K.sum(output, axis=1)


def load_all_data(symbols, start_date, end_date, interval='1m'):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
    
    Args:
        symbols (list): –°–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä
        start_date (datetime): –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
        end_date (datetime): –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
        interval (str): –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–≤–µ—á–µ–π
    
    Returns:
        pd.DataFrame: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç–µ
    symbol_data_dict = {}
    
    logging.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")
    logging.info(f"–ü–µ—Ä–∏–æ–¥: —Å {start_date} –ø–æ {end_date}, –∏–Ω—Ç–µ—Ä–≤–∞–ª: {interval}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(get_historical_data, symbol, interval, start_date, end_date): symbol 
                  for symbol in symbols}
        
        for future in futures:
            symbol = futures[future]
            try:
                logging.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
                symbol_data = future.result()
                
                if symbol_data is not None:
                    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç—ã
                    symbol_data = detect_anomalies(symbol_data)
                    symbol_data = validate_volume_confirmation(symbol_data)
                    symbol_data = remove_noise(symbol_data)
                    
                    symbol_data_dict[symbol] = symbol_data
                    logging.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(symbol_data)}")
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
                save_logs_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
    
    if not symbol_data_dict:
        error_msg = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞"
        logging.error(error_msg)
        save_logs_to_file(error_msg)
        raise ValueError(error_msg)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    try:
        logging.info("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        symbol_data_dict = calculate_cross_coin_features(symbol_data_dict)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        all_data = []
        for symbol, df in symbol_data_dict.items():
            df['symbol'] = symbol
            all_data.append(df)
        
        data = pd.concat(all_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        expected_features = ['btc_corr', 'rel_strength_btc', 'beta_btc', 'lead_lag_btc',
                           'volume_strength', 'volume_accumulation', 'is_anomaly', 
                           'clean_returns']
        
        missing_features = [f for f in expected_features if f not in data.columns]
        if missing_features:
            logging.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        initial_rows = len(data)
        data = data.dropna()
        dropped_rows = initial_rows - len(data)
        if dropped_rows > 0:
            logging.info(f"–£–¥–∞–ª–µ–Ω–æ {dropped_rows} —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
        
        logging.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        save_logs_to_file(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        
        return data
        
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}"
        logging.error(error_msg)
        save_logs_to_file(error_msg)
        raise


# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
def get_historical_data(symbols, bearish_periods, interval="1m", save_path="binance_data_bearish.csv"):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å Binance (–∞—Ä—Ö–∏–≤) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –æ–¥–∏–Ω CSV-—Ñ–∞–π–ª.

    :param symbols: —Å–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä (–ø—Ä–∏–º–µ—Ä: ['BTCUSDC', 'ETHUSDC'])
    :param bearish_periods: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–µ—Ä–∏–æ–¥–∞–º–∏ (–ø—Ä–∏–º–µ—Ä: [{"start": "2019-01-01", "end": "2019-12-31"}])
    :param interval: –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "1m" - 1 –º–∏–Ω—É—Ç–∞)
    :param save_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'binance_data_bearish.csv')
    """
    base_url_monthly = "https://data.binance.vision/data/spot/monthly/klines"
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    all_data = []
    downloaded_files = set()
    download_lock = Lock()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º threading.Lock

    def download_and_process(symbol, period):
        start_date = datetime.strptime(period["start"], "%Y-%m-%d")
        end_date = datetime.strptime(period["end"], "%Y-%m-%d")
        temp_data = []

        for current_date in pd.date_range(start=start_date, end=end_date, freq='MS'):  # MS = Monthly Start
            year = current_date.year
            month = current_date.month
            month_str = f"{month:02d}"

            file_name = f"{symbol}-{interval}-{year}-{month_str}.zip"
            file_url = f"{base_url_monthly}/{symbol}/{interval}/{file_name}"

            # –ë–ª–æ–∫–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å–∫–∞—á–∞–Ω
            with download_lock:
                if file_name in downloaded_files:
                    logging.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {file_name}, —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ

                logging.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞: {file_url}")
                response = requests.head(file_url, timeout=5)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_url}")
                    continue

                logging.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {file_url}...")
                response = requests.get(file_url, timeout=15)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_url}: –ö–æ–¥ {response.status_code}")
                    continue

                logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω {file_name}")
                downloaded_files.add(file_name)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

            try:
                zip_file = zipfile.ZipFile(BytesIO(response.content))
                csv_file = file_name.replace('.zip', '.csv')

                with zip_file.open(csv_file) as file:
                    df = pd.read_csv(file, header=None, names=[
                        "timestamp", "open", "high", "low", "close", "volume",
                        "close_time", "quote_asset_volume", "number_of_trades",
                        "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"
                    ], dtype={
                        "timestamp": "int64",
                        "open": "float32",
                        "high": "float32",
                        "low": "float32",
                        "close": "float32",
                        "volume": "float32",
                        "quote_asset_volume": "float32",
                        "number_of_trades": "int32",
                        "taker_buy_base_asset_volume": "float32",
                        "taker_buy_quote_asset_volume": "float32"
                    })

                    # üõ† –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ timestamp
                    if "timestamp" not in df.columns:
                        logging.error(f"‚ùå –û—à–∏–±–∫–∞: –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ df –¥–ª—è {symbol}")
                        return None

                    # üõ† –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime –∏ —Å—Ç–∞–≤–∏–º –∏–Ω–¥–µ–∫—Å
                    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", errors="coerce")
                    df.set_index("timestamp", inplace=True)
                    
                    df["symbol"] = symbol

                    temp_data.append(df)
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {symbol} –∑–∞ {current_date.strftime('%Y-%m')}: {e}")

            time.sleep(0.5)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è–º–∏

        return pd.concat(temp_data) if temp_data else None

    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(download_and_process, symbol, period) for symbol in symbols for period in bearish_periods]
        for future in futures:
            result = future.result()
            if result is not None:
                all_data.append(result)

    if not all_data:
        logging.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –¥–∞–Ω–Ω—ã—Ö.")
        return None

    df = pd.concat(all_data, ignore_index=False)  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º ignore_index, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å timestamp  

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å –≤ DataFrame
    logging.info(f"üìä –ö–æ–ª–æ–Ω–∫–∏ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º df: {df.columns}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å
    if "timestamp" not in df.columns and not isinstance(df.index, pd.DatetimeIndex):
        logging.error(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {df.columns}")
        return None

    # –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å resample
    df = df.resample('1min').ffill()  # –ú–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã, –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º NaN
    num_nans = df.isna().sum().sum()
    if num_nans > 0:
        nan_percentage = num_nans / len(df)
        if nan_percentage > 0.05:  # –ï—Å–ª–∏ –±–æ–ª–µ–µ 5% –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—â–µ–Ω—ã
            logging.warning(f"‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ {nan_percentage:.2%} –¥–∞–Ω–Ω—ã—Ö! –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏.")
            df.dropna(inplace=True)
        else:
            logging.info(f"üîß –ó–∞–ø–æ–ª–Ω—è–µ–º {nan_percentage:.2%} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ffill.")
            df.fillna(method='ffill', inplace=True)  # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏

    df.to_csv(save_path)
    logging.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")

    return save_path


def load_bearish_data(symbols, bearish_periods, interval="1m", save_path="binance_data_bearish.csv"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø–µ—Ä–∏–æ–¥–æ–≤.
    –ï—Å–ª–∏ —Ñ–∞–π–ª save_path —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Å —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å, –≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è DataFrame —Å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
    """
    # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äì —á–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    if os.path.exists(save_path):
        try:
            existing_data = pd.read_csv(save_path, index_col=0, parse_dates=True, on_bad_lines='skip')
            logging.info(f"–°—á–∏—Ç–∞–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {save_path}, —Å—Ç—Ä–æ–∫: {len(existing_data)}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ {save_path}: {e}")
            existing_data = pd.DataFrame()
    else:
        existing_data = pd.DataFrame()

    all_data = {}  # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–º—É —Å–∏–º–≤–æ–ª—É
    logging.info(f"üöÄ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –∑–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    with ThreadPoolExecutor(max_workers=4) as executor:
        # –ü–µ—Ä–µ–¥–∞—ë–º –≤ get_historical_data –ø–∞—Ä–∞–º–µ—Ç—Ä save_path, —á—Ç–æ–±—ã –≤—Å–µ –∑–∞–≥—Ä—É–∑–∫–∏ –∑–∞–ø–∏—Å—ã–≤–∞–ª–∏—Å—å –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
        futures = {
            executor.submit(get_historical_data, [symbol], bearish_periods, interval, save_path): symbol
            for symbol in symbols
        }
        for future in futures:
            symbol = futures[future]
            try:
                # get_historical_data –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                temp_file_path = future.result()
                if temp_file_path is not None:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º on_bad_lines='skip', —á—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
                    new_data = pd.read_csv(temp_file_path, index_col=0, parse_dates=True, on_bad_lines='skip')
                    if symbol in all_data:
                        all_data[symbol].append(new_data)
                    else:
                        all_data[symbol] = [new_data]
                    logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–ª—è {symbol}. –¢–µ–∫—É—â–∏–π —Å–ø–∏—Å–æ–∫: {len(all_data[symbol])} —Ñ–∞–π–ª–æ–≤.")
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞, –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –Ω–µ –ø—É—Å—Ç–æ–π
    for symbol in list(all_data.keys()):
        if all_data[symbol]:
            all_data[symbol] = pd.concat(all_data[symbol])
        else:
            del all_data[symbol]

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–¥–∏–Ω DataFrame
    if all_data:
        new_combined = pd.concat(all_data.values(), ignore_index=False)
    else:
        new_combined = pd.DataFrame()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–µ—Å–ª–∏ —Ç–∞–∫–æ–≤—ã–µ –∏–º–µ—é—Ç—Å—è)
    if not existing_data.empty:
        combined = pd.concat([existing_data, new_combined], ignore_index=False)
    else:
        combined = new_combined

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π DataFrame –≤ –µ–¥–∏–Ω—ã–π CSV-—Ñ–∞–π–ª
    combined.to_csv(save_path)
    logging.info(f"üíæ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path} (–∏—Ç–æ–≥–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫: {len(combined)})")

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ –∫–∞–∂–¥–æ–º—É —Å–∏–º–≤–æ–ª—É (–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–º–∏ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏)
    return all_data


'''def aggregate_to_2min(data):
    """
    –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ 1 –º–∏–Ω—É—Ç–∞ –¥–æ 2 –º–∏–Ω—É—Ç.
    
    Parameters:
        data (pd.DataFrame): –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π –≤ –∫–æ–ª–æ–Ω–∫–µ 'timestamp'.
    
    Returns:
        pd.DataFrame: –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ 'timestamp'
    if 'timestamp' not in data.columns:
        logging.error("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏.")
        if isinstance(data.index, pd.DatetimeIndex):
            data['timestamp'] = data.index
            logging.info("–ò–Ω–¥–µ–∫—Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –∫–æ–ª–æ–Ω–∫—É 'timestamp'.")
        else:
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö.")

    # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    data['timestamp'] = pd.to_datetime(data['timestamp'])  # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ datetime
    data = data.set_index('timestamp')

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    data = data.resample('2T').agg({
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum'
    }).dropna()  # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏

    logging.info(f"–ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(data)} —Å—Ç—Ä–æ–∫.")
    return data'''


def adjust_target(data, threshold=-0.0005, trend_window=50):
    """
    –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –∞–∫—Ü–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Ä–µ–∑–∫–∏—Ö –ø–∞–¥–µ–Ω–∏—è—Ö.
    
    Parameters:
        data (pd.DataFrame): –î–∞–Ω–Ω—ã–µ —Å –∫–æ–ª–æ–Ω–∫–æ–π 'returns'.
        threshold (float): –ü–æ—Ä–æ–≥ –ø–∞–¥–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, -0.05 –¥–ª—è –ø–∞–¥–µ–Ω–∏–π > 5%).
        
    Returns:
        pd.DataFrame: –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –∫–æ–ª–æ–Ω–∫–æ–π 'target'.
    """
    data['target'] = (data['returns'] < threshold).astype(int)
    data['trend'] = (data['close'] < data['close'].rolling(trend_window).mean()).astype(int)
    data['target'] = np.where(data['target'] + data['trend'] > 0, 1, 0)
    logging.info(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {data['target'].value_counts().to_dict()}")
    return data

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def extract_features(data):
    logging.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞")
    data = data.copy()
    data = remove_noise(data)

    # 1. –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Å –≥—Ä–∞–¥–∞—Ü–∏–µ–π —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–æ–≤
    returns = data['close'].pct_change()
    volume_ratio = data['volume'] / data['volume'].rolling(10).mean()
    price_acceleration = returns.diff()  # –°–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã
    
    # 2. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
    def calculate_dynamic_thresholds(window=10):
        volatility = returns.rolling(window).std()
        avg_volatility = volatility.rolling(100).mean()  # –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è —Å—Ä–µ–¥–Ω—è—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        volatility_ratio = volatility / avg_volatility
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è 1-–º–∏–Ω—É—Ç–Ω—ã—Ö —Å–≤–µ—á–µ–π –Ω–∞ –º–µ–¥–≤–µ–∂—å–µ–º —Ä—ã–Ω–∫–µ
        base_strong = -0.001  # 0.1%
        base_medium = -0.0005  # 0.05%
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ—Ä–æ–≥–æ–≤
        strong_threshold = base_strong * np.where(
            volatility_ratio > 1.5, 1.5,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é
            np.where(volatility_ratio < 0.5, 0.5, volatility_ratio)
        )
        medium_threshold = base_medium * np.where(
            volatility_ratio > 1.5, 1.5,
            np.where(volatility_ratio < 0.5, 0.5, volatility_ratio)
        )
        
        return strong_threshold, medium_threshold

    # 3. –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    strong_threshold, medium_threshold = calculate_dynamic_thresholds()
    
    # –£—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ —Ç–æ–ª—å–∫–æ —Ü–µ–Ω—É, –Ω–æ –∏ –æ–±—ä–µ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –ø–∞–¥–µ–Ω–∏—è
    data['target'] = np.where(
        (returns.shift(-1) < strong_threshold) & 
        (volume_ratio > 1.2) & 
        (price_acceleration < 0) &
        (data['volume'] > data['volume'].rolling(20).mean()), 
        2,  # –°–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª
        np.where(
            (returns.shift(-1) < medium_threshold) & 
            (volume_ratio > 1) & 
            (price_acceleration < 0),
            1,  # –°—Ä–µ–¥–Ω–∏–π —Å–∏–≥–Ω–∞–ª
            0   # –ù–µ—Ç —Å–∏–≥–Ω–∞–ª–∞
        )
    )

    # 2. –ë–∞–∑–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    data['returns'] = returns
    data['log_returns'] = np.log(data['close'] / data['close'].shift(1))

    # 3. –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤ –∏ –¥–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–¥–∞–∂ (—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞ –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞)
    data['volume_ma'] = data['volume'].rolling(10).mean()
    data['volume_ratio'] = data['volume'] / data['volume_ma']
    data['selling_pressure'] = data['volume'] * (data['close'] - data['open']).abs() * \
                              np.where(data['close'] < data['open'], 1, 0)
    data['buying_pressure'] = data['volume'] * (data['close'] - data['open']).abs() * \
                             np.where(data['close'] > data['open'], 1, 0)
    data['pressure_ratio'] = data['selling_pressure'] / data['buying_pressure'].replace(0, 1)

    # 4. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
    data['volatility'] = returns.rolling(10).std()
    data['volatility_ma'] = data['volatility'].rolling(20).mean()
    data['volatility_ratio'] = data['volatility'] / data['volatility_ma']

    # 5. –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–µ—Ä–∏–æ–¥–∞–º–∏
    for period in [3, 5, 8, 13, 21]:  # –ß–∏—Å–ª–∞ –§–∏–±–æ–Ω–∞—á—á–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ö–≤–∞—Ç–∞
        data[f'sma_{period}'] = SMAIndicator(data['smoothed_close'], window=period).sma_indicator()
        data[f'ema_{period}'] = data['smoothed_close'].ewm(span=period, adjust=False).mean()

    # 6. MACD —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–æ–¥ –º–∏–Ω—É—Ç–Ω—ã–µ —Å–≤–µ—á–∏
    macd = MACD(data['smoothed_close'], window_slow=26, window_fast=12, window_sign=9)
    data['macd'] = macd.macd()
    data['macd_signal'] = macd.macd_signal()
    data['macd_diff'] = data['macd'] - data['macd_signal']
    data['macd_slope'] = data['macd_diff'].diff()  # –°–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è MACD

    # 7. –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –º–µ–¥–≤–µ–∂–∏–π —Ä—ã–Ω–æ–∫
    data['obv'] = OnBalanceVolumeIndicator(data['close'], data['volume']).on_balance_volume()
    data['cmf'] = ChaikinMoneyFlowIndicator(data['high'], data['low'], data['close'], data['volume']).chaikin_money_flow()
    data['volume_change'] = data['volume'].pct_change()
    data['volume_ma_ratio'] = data['volume'] / data['volume'].rolling(20).mean()

    # 8. –û—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–µ—Ä–∏–æ–¥–∞–º–∏
    for period in [7, 14, 21]:
        data[f'rsi_{period}'] = RSIIndicator(data['close'], window=period).rsi()
    data['stoch_k'] = StochasticOscillator(data['high'], data['low'], data['close'], window=7).stoch()
    data['stoch_d'] = StochasticOscillator(data['high'], data['low'], data['close'], window=7).stoch_signal()
    
    # 9. –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
    data['support_level'] = data['low'].rolling(20).min()
    data['resistance_level'] = data['high'].rolling(20).max()
    data['price_to_support'] = data['close'] / data['support_level']
    
    # 10. –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π –∏ –∏—Ö —Å–∏–ª—ã
    data['candle_body'] = abs(data['close'] - data['open'])
    data['upper_shadow'] = data['high'] - np.maximum(data['close'], data['open'])
    data['lower_shadow'] = np.minimum(data['close'], data['open']) - data['low']
    data['body_to_shadow_ratio'] = data['candle_body'] / (data['upper_shadow'] + data['lower_shadow']).replace(0, 0.001)

    # 11. –¶–µ–Ω–æ–≤—ã–µ —É—Ä–æ–≤–Ω–∏ –∏ –∏—Ö –ø—Ä–æ—Ä—ã–≤—ã
    data['price_level_breach'] = np.where(
        data['close'] < data['support_level'].shift(1), -1,
        np.where(data['close'] > data['resistance_level'].shift(1), 1, 0)
    )

    # 12. –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–∫–æ—Ä–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è
    data['price_acceleration'] = returns.diff()
    data['volume_acceleration'] = data['volume_change'].diff()
    
    # 13. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (—Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–µ—Ä–∏–æ–¥–∞–º–∏)
    bb = BollingerBands(data['smoothed_close'], window=20)
    data['bb_high'] = bb.bollinger_hband()
    data['bb_low'] = bb.bollinger_lband()
    data['bb_width'] = bb.bollinger_wband()
    data['bb_position'] = (data['close'] - data['bb_low']) / (data['bb_high'] - data['bb_low'])
    
    # 14. ATR —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–µ—Ä–∏–æ–¥–∞–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
    for period in [5, 10, 20]:
        data[f'atr_{period}'] = AverageTrueRange(
            data['high'], data['low'], data['close'], window=period
        ).average_true_range()

    # 4. –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏
    data['micro_trend'] = np.where(
        data['smoothed_close'] > data['smoothed_close'].shift(1), 1,
        np.where(data['smoothed_close'] < data['smoothed_close'].shift(1), -1, 0)
    )
    
    # –°—á–∏—Ç–∞–µ–º –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥—ã –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5 –º–∏–Ω—É—Ç
    data['micro_trend_sum'] = data['micro_trend'].rolling(5).sum()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫ —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—ä–µ–º–æ–≤
    data['volume_acceleration_5m'] = (
        data['volume'].diff() / data['volume'].rolling(5).mean()
    ).fillna(0)

    # 5. –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–∏–ª—ã –º–µ–¥–≤–µ–∂—å–µ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è
    data['bearish_strength'] = np.where(
        (data['close'] < data['open']) &  # –ú–µ–¥–≤–µ–∂—å—è —Å–≤–µ—á–∞
        (data['volume'] > data['volume'].rolling(20).mean() * 1.5) &  # –û–±—ä–µ–º –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
        (data['close'] == data['low']) &  # –ó–∞–∫—Ä—ã—Ç–∏–µ –Ω–∞ –º–∏–Ω–∏–º—É–º–µ
        (data['clean_returns'] < 0),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ returns
        3,  # –°–∏–ª—å–Ω–æ–µ –º–µ–¥–≤–µ–∂—å–µ –¥–≤–∏–∂–µ–Ω–∏–µ
        np.where(
            (data['close'] < data['open']) &
            (data['volume'] > data['volume'].rolling(20).mean()) &
            (data['clean_returns'] < 0),
            2,  # –°—Ä–µ–¥–Ω–µ–µ –º–µ–¥–≤–µ–∂—å–µ –¥–≤–∏–∂–µ–Ω–∏–µ
            np.where(data['close'] < data['open'], 1, 0)  # –°–ª–∞–±–æ–µ/–Ω–µ—Ç –¥–≤–∏–∂–µ–Ω–∏—è
        )
    )
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features = {}
    
    features['target'] = data['target']  # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    
    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤—Å–µ, —á—Ç–æ —É–∂–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ)
    for col in data.columns:
        if col not in ['market_type']:
            features[col] = data[col]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if 'btc_corr' in data.columns:
        features['btc_corr'] = data['btc_corr']
    if 'rel_strength_btc' in data.columns:
        features['rel_strength_btc'] = data['rel_strength_btc']
    if 'beta_btc' in data.columns:
        features['beta_btc'] = data['beta_btc']
            
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –æ–±—ä–µ–º–æ–º, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if 'volume_strength' in data.columns:
        features['volume_strength'] = data['volume_strength']
    if 'volume_accumulation' in data.columns:
        features['volume_accumulation'] = data['volume_accumulation']
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –æ—Ç —à—É–º–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if 'clean_returns' in data.columns:
        features['clean_returns'] = data['clean_returns']
        
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ DataFrame
    features_df = pd.DataFrame(features)
    
    # –î–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω–µ—Ü —Ñ—É–Ω–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ return
    logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features_df.columns)}")
    logging.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN: {features_df.isna().sum().sum()}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:\n{features_df['target'].value_counts()}")
    
    logging.info(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {list(data.columns)}")
    num_nans = data.isna().sum().sum()
    if num_nans > 0:
        logging.warning(f"‚ö† –ù–∞–π–¥–µ–Ω–æ {num_nans} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π. –ó–∞–ø–æ–ª–Ω—è–µ–º...")
        data.fillna(0, inplace=True)


    return features_df.replace([np.inf, -np.inf], np.nan).ffill().bfill()


def remove_outliers(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    return data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]


def add_clustering_feature(data):
    features_for_clustering = [
        'close', 'volume', 'rsi', 'macd', 'atr', 'sma_10', 'sma_30', 'ema_50', 'ema_200',
        'bb_width', 'macd_diff', 'obv', 'returns', 'log_returns'
    ]
    kmeans = KMeans(n_clusters=5, random_state=42)
    data['cluster'] = kmeans.fit_predict(data[features_for_clustering])
    return data

def prepare_data(data):
    logging.info("–ù–∞—á–∞–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
    if data.empty:
        raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã")
        
    logging.info(f"–ò—Å—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {data.shape}")
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    if not isinstance(data.index, pd.DatetimeIndex):
        if 'timestamp' in data.columns:
            data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')
            data.set_index('timestamp', inplace=True)
        else:
            raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ 'timestamp'.")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    data = extract_features(data)
    logging.info(f"–ü–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {data.shape}")
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
    data = remove_outliers(data)
    logging.info(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤: {data.shape}")
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
    data = add_clustering_feature(data)
    logging.info(f"–ü–æ—Å–ª–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {data.shape}")
    
    # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –ò–°–ö–õ–Æ–ß–ê–ï–ú timestamp –∏ –¥—Ä—É–≥–∏–µ –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    features = [col for col in data.columns if col not in ['target', 'timestamp'] and pd.api.types.is_numeric_dtype(data[col])]
    logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}")
    logging.info(f"–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target:\n{data['target'].value_counts()}")
    
    return data, features

def clean_data(X, y):
    logging.info("–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ NaN")
    mask = np.isfinite(X).all(axis=1)
    X_clean = X[mask]
    y_clean = y[mask]
    return X_clean, y_clean


def load_last_saved_model(model_filename):
    try:
        models = sorted(
            [f for f in os.listdir() if f.startswith(model_filename)],
            key=lambda x: int(x.split('_')[-1].split('.')[0])
        )
        last_model = models[-1] if models else None
        if last_model:
            logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–µ–ª–∏: {last_model}")
            return load_model(last_model)
        else:
            return None
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–µ–ª–∏: {e}")
        return None

def balance_classes(X, y):
    logging.info("–ù–∞—á–∞–ª–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤")
    logging.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={X.shape}, y={y.shape}")
    logging.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –≤ y: {np.unique(y, return_counts=True)}")

    if X.shape[0] == 0 or y.shape[0] == 0:
        raise ValueError("–î–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –ø—É—Å—Ç—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ñ–∏–ª—å—Ç—Ä—ã.")

    smote_tomek = SMOTETomek(random_state=42)
    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)

    logging.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={X_resampled.shape}, y={y_resampled.shape}")
    return X_resampled, y_resampled

def train_xgboost_on_embeddings(X_emb, y):
    """
    –û–±—É—á–∞–µ—Ç XGBoost-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö –∏–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è y –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ {0, 1, 2}.
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞, –Ω–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ
    –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ª—É—á—à–µ–π –ø–æ–¥–≥–æ–Ω–∫–∏ –ø–æ–¥ –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ.
    """
    logging.info("–û–±—É—á–µ–Ω–∏–µ XGBoost –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö...")
    xgb_model = XGBClassifier(
        objective='multi:softprob',  # –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∑–∞–¥–∞—á–∞
        n_estimators=10,
        max_depth=3,
        learning_rate=0.01,
        random_state=42,
        num_class=3  # 3 –∫–ª–∞—Å—Å–∞
    )
    xgb_model.fit(X_emb, y)
    logging.info("XGBoost –æ–±—É—á–µ–Ω –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö.")
    return xgb_model



def prepare_timestamp_column(data):
    """
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ–∑–¥–∞—ë—Ç —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –¥–ª—è DataFrame.
    
    –ê–ª–≥–æ—Ä–∏—Ç–º:
      1. –ï—Å–ª–∏ —Å—Ç–æ–ª–±–µ—Ü 'timestamp' —É–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –æ–Ω —É–¥–∞–ª—è–µ—Ç—Å—è.
      2. –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è data.reset_index(), —á—Ç–æ–±—ã –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å (–∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å DatetimeIndex)
         –≤ —Å—Ç–æ–ª–±–µ—Ü. –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –∏–º–µ–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–º—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'timestamp' –∏–ª–∏ –¥—Ä—É–≥–æ–µ), –µ–≥–æ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –≤ 'timestamp'.
      3. –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è ‚Äî DataFrame, –≥–¥–µ —Å—Ç–æ–ª–±–µ—Ü 'timestamp' —Ç–æ—á–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç.
      
    –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ–∑–¥–∞—ë—Ç –Ω—É–∂–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü –±–µ–∑ —Ä–∏—Å–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è.
    """
    logging.info("–£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—è reset_index().")
    
    # –ï—Å–ª–∏ 'timestamp' —É–∂–µ –µ—Å—Ç—å, —É–¥–∞–ª—è–µ–º –µ–≥–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    if 'timestamp' in data.columns:
        logging.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω —Å—Ç–æ–ª–±–µ—Ü 'timestamp'. –£–¥–∞–ª—è–µ–º –µ–≥–æ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ.")
        data = data.drop(columns=['timestamp'])
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å: –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex, —Ç–æ –æ–Ω –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—Å—è –≤ –∫–æ–ª–æ–Ω–∫—É
    data = data.reset_index()
    
    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Å–±—Ä–æ—Å–∞ –∏–Ω–¥–µ–∫—Å –Ω–∞–∑—ã–≤–∞–ª—Å—è –Ω–µ 'timestamp', –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –µ–≥–æ
    if 'timestamp' not in data.columns:
        # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —Å–±—Ä–æ—à–µ–Ω –∫–∞–∫ 'index', —Ç–æ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –µ–≥–æ –≤ 'timestamp'
        if 'index' in data.columns:
            data.rename(columns={'index': 'timestamp'}, inplace=True)
            logging.info("–ö–æ–ª–æ–Ω–∫–∞ 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –≤ 'timestamp'.")
        else:
            # –ï—Å–ª–∏ –Ω–∏ 'timestamp', –Ω–∏ 'index' –Ω–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç, —Å–æ–∑–¥–∞—ë–º —Å—Ç–æ–ª–±–µ—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
            data['timestamp'] = data.index
            logging.info("–°—Ç–æ–ª–±–µ—Ü 'timestamp' —Å–æ–∑–¥–∞–Ω –∏–∑ –∏–Ω–¥–µ–∫—Å–∞.")
    else:
        # –ü—Ä–∏–≤–æ–¥–∏–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Å—Ç–æ–ª–±–µ—Ü –∫ —Ç–∏–ø—É datetime
        data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')
        logging.info("–°—Ç–æ–ª–±–µ—Ü 'timestamp' –ø—Ä–∏–≤–µ–¥—ë–Ω –∫ —Ç–∏–ø—É datetime.")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Å—Ç–∞–≤–∏—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –Ω–∞ –ø–µ—Ä–≤—É—é –ø–æ–∑–∏—Ü–∏—é, –µ—Å–ª–∏ —ç—Ç–æ –Ω—É–∂–Ω–æ
    cols = list(data.columns)
    if cols[0] != 'timestamp':
        cols.insert(0, cols.pop(cols.index('timestamp')))
        data = data[cols]
        logging.info("–°—Ç–æ–ª–±–µ—Ü 'timestamp' –ø–µ—Ä–µ—Å—Ç–∞–≤–ª–µ–Ω –≤ –Ω–∞—á–∞–ª–æ DataFrame.")
    
    return data


def build_bearish_neural_network(data):
    """
    –û–±—É—á–∞–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∏.
    
    –ó–¥–µ—Å—å –≤ —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ prepare_timestamp_column,
    –∫–æ—Ç–æ—Ä–∞—è —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ —Å–æ–∑–¥–∞—ë—Ç (–∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç) —Å—Ç–æ–ª–±–µ—Ü 'timestamp'. 
    –î–∞–ª–µ–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞, —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—ã–±–æ—Ä–æ–∫, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏,
    —Å–æ—Ö—Ä–∞–Ω—è—è –≤–µ—Å—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª (–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É LSTM, –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å XGBoost –∏ –ø—Ä.).
    
    –í–∞–∂–Ω–æ: —Ç–µ–ø–µ—Ä—å –Ω–∏–∫–∞–∫–∏–µ reset_index() –Ω–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ø–æ—Å–ª–µ –≤—ã–∑–æ–≤–∞ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.
    """
    logging.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞.")
    
    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ–∑–¥–∞—ë–º —Å—Ç–æ–ª–±–µ—Ü 'timestamp'
    data = prepare_timestamp_column(data)
    
    # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏—Å–∫–ª—é—á–∞—è 'target' –∏ 'timestamp')
    selected_features = [
        col for col in data.columns 
        if col not in ['target', 'timestamp'] and pd.api.types.is_numeric_dtype(data[col])
    ]
    y = data['target'].copy()
    X = data[selected_features].copy()
    
    logging.info(f"–†–∞–∑–º–µ—Ä X –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {X.shape}")
    logging.info(f"–†–∞–∑–º–µ—Ä y –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {y.shape}")
    logging.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è y: {np.unique(y, return_counts=True)}")
    
    X = X.astype(float)
    y = y.astype(int)
    mask = ~np.isnan(y)
    X = X[mask]
    y = y[mask]
    if X.size == 0 or y.size == 0:
        logging.error("X –∏–ª–∏ y –ø—É—Å—Ç—ã –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö.")
        raise ValueError("X –∏–ª–∏ y –ø—É—Å—Ç—ã –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN.")
    logging.info(f"–†–∞–∑–º–µ—Ä X –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {X.shape}")
    logging.info(f"–†–∞–∑–º–µ—Ä y –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {y.shape}")
    
    X_resampled, y_resampled = balance_classes(X, y)
    logging.info(f"–†–∞–∑–º–µ—Ä—ã –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X_resampled={X_resampled.shape}, y_resampled={y_resampled.shape}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:\n{pd.Series(y_resampled).value_counts()}")
    X_resampled = pd.DataFrame(X_resampled, columns=X.columns)
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
    )
    logging.info(f"–†–∞–∑–º–µ—Ä—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: X_train={X_train.shape}, y_train={y_train.shape}")
    logging.info(f"–†–∞–∑–º–µ—Ä—ã –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: X_val={X_val.shape}, y_val={y_val.shape}")
    
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
    X_val_scaled = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
    
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train)).batch(32).prefetch(tf.data.AUTOTUNE)
    val_dataset = tf.data.Dataset.from_tensor_slices((X_val_scaled, y_val)).batch(32).prefetch(tf.data.AUTOTUNE)
    
    def hft_metrics(y_true, y_pred):
        reaction_time = tf.reduce_mean(tf.abs(y_pred[1:] - y_pred[:-1]))
        signal_stability = tf.reduce_mean(tf.abs(y_pred[2:] - 2 * y_pred[1:-1] + y_pred[:-2]))
        return reaction_time, signal_stability
    
    def profit_ratio(y_true, y_pred):
        successful_shorts = tf.reduce_sum(tf.where(tf.logical_and(y_true >= 1, y_pred >= 0.5), 1.0, 0.0))
        false_signals = tf.reduce_sum(tf.where(tf.logical_and(y_true == 0, y_pred >= 0.5), 1.0, 0.0))
        return successful_shorts / (false_signals + K.epsilon())
    
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            strategy = tf.distribute.MirroredStrategy()
            logging.info("GPU –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MirroredStrategy")
        except RuntimeError as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU: {e}")
            strategy = tf.distribute.get_strategy()
    else:
        strategy = tf.distribute.get_strategy()
        logging.info("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    logging.info("–ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞...")
    with strategy.scope():
        inputs = Input(shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]))
        x1 = LSTM(256, return_sequences=True)(inputs)
        x1 = BatchNormalization()(x1)
        x1 = Dropout(0.3)(x1)
        x2 = LSTM(256, return_sequences=True)(inputs)
        x2 = BatchNormalization()(x2)
        x2 = Dropout(0.3)(x2)
        x3 = LSTM(256, return_sequences=True, name='market_context')(inputs)
        x3 = BatchNormalization()(x3)
        x3 = Dropout(0.3)(x3)
        x = Add()([x1, x2, x3])
        x = LSTM(256, return_sequences=False)(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        x = Dense(128, activation='relu', name="embedding_layer")(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        x = Dense(256, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        outputs = Dense(3, activation='softmax')(x)
        model = tf.keras.models.Model(inputs, outputs)
    
        model.compile(optimizer=Adam(learning_rate=0.001),
                      loss=custom_profit_loss,
                      metrics=[hft_metrics, profit_ratio])
    
        try:
            model.load_weights(checkpoint_path_regular.format(epoch=0))
            logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {checkpoint_path_regular.format(epoch=0)}")
        except FileNotFoundError:
            logging.info("–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
    
        regular_checkpoints = sorted(glob.glob(f"checkpoints/{network_name}_checkpoint_epoch_*.h5"))
        if regular_checkpoints:
            latest_checkpoint = regular_checkpoints[-1]
            try:
                model.load_weights(latest_checkpoint)
                logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {latest_checkpoint}")
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
    
        if os.path.exists(checkpoint_path_best):
            try:
                model.load_weights(checkpoint_path_best)
                logging.info(f"–õ—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –Ω–∞–π–¥–µ–Ω: {checkpoint_path_best}. –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã.")
            except Exception as e:
                logging.info("–õ—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –ø–æ–∫–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω. –≠—Ç–æ –æ–∂–∏–¥–∞–µ–º–æ, –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –µ—â—ë –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
    
        checkpoint_every_epoch = ModelCheckpoint(filepath=checkpoint_path_regular,
                                                 save_weights_only=True,
                                                 save_best_only=False,
                                                 verbose=1)
        checkpoint_best_model = ModelCheckpoint(filepath=checkpoint_path_best,
                                                save_weights_only=True,
                                                save_best_only=True,
                                                monitor='val_loss',
                                                verbose=1)
        tensorboard_callback = TensorBoard(log_dir=f"logs/{time.time()}")
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,
                                      patience=5, min_lr=1e-5, verbose=1)
        early_stopping = EarlyStopping(monitor='val_loss', patience=5,
                                       restore_best_weights=True, mode='min')
    
        history = model.fit(train_dataset,
                            epochs=200,
                            validation_data=val_dataset,
                            class_weight={0: 1.0, 1: 2.0, 2: 3.0},
                            verbose=1,
                            callbacks=[early_stopping, checkpoint_every_epoch,
                                       checkpoint_best_model, tensorboard_callback,
                                       reduce_lr])
    
        for checkpoint in glob.glob(f"checkpoints/{network_name}_checkpoint_epoch_*.h5"):
            if checkpoint != checkpoint_path_best:
                os.remove(checkpoint)
                logging.info(f"–£–¥–∞–ª—ë–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint}")
        logging.info("–û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å.")
    
        plt.figure(figsize=(10, 6))
        plt.plot(history.history['loss'], label='Train Loss', color='blue')
        plt.plot(history.history['val_loss'], label='Validation Loss', color='orange', linestyle='--')
        plt.title('Train vs Validation Loss')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.grid(True)
        plt.legend()
        plt.show()
    
        try:
            model.save("bearish_neural_network.h5")
            logging.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'bearish_neural_network.h5'")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
    
        logging.info("–≠—Ç–∞–ø –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏–µ XGBoost –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞.")
        try:
            feature_extractor = Model(inputs=model.input, outputs=model.get_layer("embedding_layer").output)
            embeddings_train = feature_extractor.predict(X_train_scaled)
            embeddings_val = feature_extractor.predict(X_val_scaled)
            logging.info(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–ª—É—á–µ–Ω—ã: embeddings_train.shape = {embeddings_train.shape}")
    
            xgb_model = train_xgboost_on_embeddings(embeddings_train, y_train)
            logging.info("XGBoost –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö.")
    
            nn_val_pred = model.predict(X_val_scaled)
            xgb_val_pred = xgb_model.predict_proba(embeddings_val)
            ensemble_val_pred = 0.5 * nn_val_pred + 0.5 * xgb_val_pred
            ensemble_val_pred_class = np.argmax(ensemble_val_pred, axis=1)
            ensemble_f1 = f1_score(y_val, ensemble_val_pred_class, average='weighted')
            logging.info(f"–≠—Ç–∞–ø –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è: F1-score –∞–Ω—Å–∞–º–±–ª—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ = {ensemble_f1:.4f}")
    
            joblib.dump(xgb_model, "xgb_model_bearish.pkl")
            logging.info("XGBoost-–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'xgb_model_bearish.pkl'")
    
            ensemble_model = {"nn_model": model,
                              "xgb_model": xgb_model,
                              "feature_extractor": feature_extractor,
                              "ensemble_weight_nn": 0.5,
                              "ensemble_weight_xgb": 0.5}
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            ensemble_model = {"nn_model": model}
    
        return {"ensemble_model": ensemble_model, "scaler": scaler}

if __name__ == "__main__":
    try:
        strategy = initialize_strategy()
        
        symbols = ['BTCUSDC', 'ETHUSDC', 'BNBUSDC','XRPUSDC', 'ADAUSDC', 'SOLUSDC', 'DOTUSDC', 'LINKUSDC', 'TONUSDC', 'NEARUSDC']
        
        bearish_periods = [
            {"start": "2018-01-17", "end": "2018-03-31"},
            {"start": "2018-09-01", "end": "2018-12-31"},
            {"start": "2021-05-12", "end": "2021-08-31"},
            {"start": "2022-05-01", "end": "2022-07-31"},
            {"start": "2022-09-01", "end": "2022-12-15"},
            {"start": "2022-12-16", "end": "2023-01-31"}
        ]
        
        logging.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞...")
        data_dict = load_bearish_data(symbols, bearish_periods, interval="1m")
        if not data_dict:
            raise ValueError("‚ùå –û—à–∏–±–∫–∞: –î–∞–Ω–Ω—ã–µ –Ω–µ –±—ã–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        data = pd.concat(data_dict.values(), ignore_index=False)
        if data.empty:
            raise ValueError("‚ùå –û—à–∏–±–∫–∞: –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã!")
        # –ó–¥–µ—Å—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–∑—ã–≤–∞–µ–º prepare_timestamp_column, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'timestamp'
        data = prepare_timestamp_column(data)
        logging.info(f"‚Ñπ –ü–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Å—Ç–æ–ª–±—Ü–∞, –∫–æ–ª–æ–Ω–∫–∏: {data.columns.tolist()}")
        logging.info(f"üìà –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏: {data.shape}")
        logging.info("üõ† –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö...")
        data = extract_features(data)
        data.dropna(inplace=True)
        data = data.loc[:, ~data.columns.duplicated()]
        if data.empty:
            raise ValueError("‚ùå –û—à–∏–±–∫–∞: –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã!")
        logging.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞...")
        build_bearish_neural_network(data)
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã: {e}")
    finally:
        logging.info("üóë –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        cleanup_training_files()
        logging.info("üßπ –û—á–∏—Å—Ç–∫–∞ —Å–µ—Å—Å–∏–∏ TensorFlow...")
        clear_session()
        logging.info("‚úÖ –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    sys.exit(0)
