import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, log_loss
from imblearn.combine import SMOTETomek
from joblib import Parallel, delayed
from binance.client import Client
from datetime import datetime, timedelta
import os
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.cluster import KMeans
from concurrent.futures import ThreadPoolExecutor
from ta.trend import SMAIndicator, MACD, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator, WilliamsRIndicator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator, ChaikinMoneyFlowIndicator, AccDistIndexIndicator
from sklearn.utils.class_weight import compute_class_weight
from ta.trend import ADXIndicator
import sys
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, precision_score, recall_score
from filterpy.kalman import KalmanFilter
import requests
import zipfile
from io import BytesIO
import tensorflow as tf
from threading import Lock
from joblib import parallel_backend
import stat
from sklearn.base import clone
from datetime import datetime
import joblib, glob, shutil, logging, dill, time
from datetime import datetime
  

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("debug_log_bullish_ensemble.log", encoding="utf-8"),
        logging.StreamHandler(sys.stdout)  # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —é–Ω–∏–∫–æ–¥–∞
    ]
)

# –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
market_type = "bearish"

ensemble_model_filename = 'models/bearish_stacked_ensemble_model.pkl'

checkpoint_base_dir = f"checkpoints/{market_type}"

ensemble_checkpoint_path = os.path.join(checkpoint_base_dir, f"{market_type}_ensemble_checkpoint.pkl")


def initialize_strategy():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è GPU, –µ—Å–ª–∏ –æ–Ω–∏ –¥–æ—Å—Ç—É–ø–Ω—ã.
    –ï—Å–ª–∏ GPU –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é (CPU –∏–ª–∏ –æ–¥–∏–Ω GPU, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å).
    """
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –≤–∫–ª—é—á–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏,
            # —á—Ç–æ–±—ã TensorFlow –Ω–µ –∑–∞–Ω–∏–º–∞–ª –≤—Å—é –ø–∞–º—è—Ç—å —Å—Ä–∞–∑—É
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            strategy = tf.distribute.MirroredStrategy()  # –†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU
            print("Running on GPU(s) with strategy:", strategy)
        except RuntimeError as e:
            print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:", e)
            strategy = tf.distribute.get_strategy()
    else:
        print("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.")
        strategy = tf.distribute.get_strategy()
    return strategy


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
# –§—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (exist_ok=True)
def ensure_directory(path):
    os.makedirs(path, exist_ok=True)

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ —É–¥–∞–ª–µ–Ω–∏—è
def _on_rm_error(func, path, exc_info):
    try:
        os.chmod(path, stat.S_IWRITE)
        func(path)
    except Exception as e:
        logging.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {path}: {e}")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø—É—Ç–∏ –∫ —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º
def get_checkpoint_path(model_name, market_type):
    cp_path = os.path.join("checkpoints", market_type, model_name)
    ensure_directory(cp_path)
    return cp_path
        

def save_logs_to_file(message):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–≥–∏ –≤ —Ñ–∞–π–ª.
    """
    with open("trading_logs.txt", "a") as f:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        f.write(f"{timestamp} - {message}\n")
        
        
def calculate_cross_coin_features(data_dict):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä.
    
    Args:
        data_dict (dict): –°–ª–æ–≤–∞—Ä—å DataFrame'–æ–≤ –ø–æ –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç–µ
    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å DataFrame'–æ–≤ —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    btc_data = data_dict['BTCUSDC']
    
    for symbol, df in data_dict.items():
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å BTC
        df['btc_corr'] = df['close'].rolling(30).corr(btc_data['close'])
        
        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞ –∫ BTC
        df['rel_strength_btc'] = (df['close'].pct_change() - 
                                btc_data['close'].pct_change())
        
        # –ë–µ—Ç–∞ –∫ BTC
        df['beta_btc'] = (df['close'].pct_change().rolling(30).cov(
            btc_data['close'].pct_change()) / 
            btc_data['close'].pct_change().rolling(30).var())
        
        # –û–ø–µ—Ä–µ–∂–µ–Ω–∏–µ/—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞ BTC
        df['lead_lag_btc'] = df['close'].pct_change().shift(-1).rolling(10).corr(
            btc_data['close'].pct_change())
            
        data_dict[symbol] = df
        
    return data_dict

def detect_anomalies(data):
    """
    –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –∏ –ø–æ–º–µ—á–∞–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Å–≤–µ—á–∏.
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Å—Ç–æ–ª–±—Ü—ã 'close' –∏ 'volume' –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.
    """
    data = data.copy()
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É
    for col in ['close', 'volume']:
        data[col] = pd.to_numeric(data[col], errors='coerce')
    
    data['volume_zscore'] = (data['volume'] - data['volume'].rolling(50).mean()) / data['volume'].rolling(50).std()
    data['price_zscore'] = (data['close'] - data['close'].rolling(50).mean()) / data['close'].rolling(50).std()
    data['is_anomaly'] = (((abs(data['volume_zscore']) > 3) & (data['close'] < data['close'].shift(1))) | 
                          (abs(data['price_zscore']) > 3))
    return data


def validate_volume_confirmation(data):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –æ–±—ä–µ–º–æ–º.
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Å—Ç–æ–ª–±—Ü—ã 'close' –∏ 'volume' –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.
    """
    data = data.copy()
    for col in ['close', 'volume']:
        data[col] = pd.to_numeric(data[col], errors='coerce')
        
    data['volume_trend_conf'] = np.where(
        (data['close'] > data['close'].shift(1)) & 
        (data['volume'] > data['volume'].rolling(20).mean()),
        1,
        np.where(
            (data['close'] < data['close'].shift(1)) & 
            (data['volume'] > data['volume'].rolling(20).mean()),
            -1,
            0
        )
    )
    data['volume_strength'] = data['volume'] / data['volume'].rolling(20).mean() * data['volume_trend_conf']
    data['volume_accumulation'] = data['volume_trend_conf'].rolling(5).sum()
    return data



def remove_noise(data):
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞ –∏ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è.
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Å—Ç–æ–ª–±–µ—Ü 'close' –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞.
    """
    data = data.copy()
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ 'close' –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É
    data['close'] = pd.to_numeric(data['close'], errors='coerce')
    
    kf = KalmanFilter(dim_x=2, dim_z=1)
    kf.x = np.array([[data['close'].iloc[0]], [0.]])
    kf.F = np.array([[1., 1.], [0., 1.]])
    kf.H = np.array([[1., 0.]])
    kf.P *= 10
    kf.R = 5
    kf.Q = np.array([[0.1, 0.1], [0.1, 0.1]])
    
    smoothed_prices = []
    for price in data['close']:
        kf.predict()
        kf.update(price)
        smoothed_prices.append(float(kf.x[0]))
    data['smoothed_close'] = smoothed_prices
    
    ema_smooth = data['close'].ewm(span=10, min_periods=1, adjust=False).mean()
    rolling_std = data['close'].rolling(window=20).std()
    rolling_mean = data['close'].rolling(window=20).mean()
    
    data['is_anomaly'] = (abs(data['close'] - rolling_mean) > (3 * rolling_std)).astype(int)
    # –£–∫–∞–∑—ã–≤–∞–µ–º fill_method=None –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è FutureWarning
    data['clean_returns'] = data['smoothed_close'].pct_change(fill_method=None) * (1 - data['is_anomaly'])
    
    data = data.ffill().bfill()
    return data



def preprocess_market_data(data_dict):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π.
    """
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    data_dict = calculate_cross_coin_features(data_dict)
    
    for symbol, df in data_dict.items():
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
        df = detect_anomalies(df)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–º
        df = validate_volume_confirmation(df)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —à—É–º
        df = remove_noise(df)
        
        data_dict[symbol] = df
    
    return data_dict


# ------------------ CheckpointGradientBoosting ------------------
class CheckpointGradientBoosting(GradientBoostingClassifier):
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3,
                 random_state=None, subsample=1.0, min_samples_split=2,
                 min_samples_leaf=1, **kwargs):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º _checkpoint_dir, –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω, –∏–Ω–∞—á–µ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –ø—É—Ç—å
        self._checkpoint_dir = kwargs.pop('_checkpoint_dir', get_checkpoint_path("gradient_boosting", market_type))
        super().__init__(n_estimators=n_estimators, learning_rate=learning_rate,
                         max_depth=max_depth, random_state=random_state,
                         subsample=subsample, min_samples_split=min_samples_split,
                         min_samples_leaf=min_samples_leaf, **kwargs)

    def __getstate__(self):
        state = self.__dict__.copy()
        state.pop('_checkpoint_dir', None)
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._checkpoint_dir = get_checkpoint_path("gradient_boosting", market_type)

    def get_params(self, deep=True):
        params = super().get_params(deep=deep)
        params.pop('_checkpoint_dir', None)
        return params

    def fit(self, X, y):
        logging.info("[GradientBoosting] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏")
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        n_features = X.shape[1]
        existing_stages = []
        for i in range(self.n_estimators):
            cp_path = os.path.join(self._checkpoint_dir, f"gradient_boosting_checkpoint_{i+1}.pkl")
            if os.path.exists(cp_path):
                try:
                    with open(cp_path, 'rb') as f:
                        stage = dill.load(f)
                    if hasattr(stage, 'n_features_') and stage.n_features_ == n_features:
                        existing_stages.append(stage)
                        logging.info(f"[GradientBoosting] –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—è {i+1} –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
                except Exception as e:
                    logging.warning(f"[GradientBoosting] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç {i+1}: {e}")
        if not existing_stages:
            if os.path.exists(self._checkpoint_dir):
                shutil.rmtree(self._checkpoint_dir, onerror=_on_rm_error)
            ensure_directory(self._checkpoint_dir)
            logging.info("[GradientBoosting] –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")
            super().fit(X, y)
        else:
            self.estimators_ = existing_stages
            remaining = self.n_estimators - len(existing_stages)
            if remaining > 0:
                orig_n_classes = self.n_classes_
                self.n_estimators = remaining
                super().fit(X, y)
                self.n_classes_ = orig_n_classes
                self.estimators_.extend(existing_stages)
                self.n_estimators = len(self.estimators_)
        for i, stage in enumerate(self.estimators_):
            cp_path = os.path.join(self._checkpoint_dir, f"gradient_boosting_checkpoint_{i+1}.pkl")
            if not os.path.exists(cp_path):
                try:
                    with open(cp_path, 'wb') as f:
                        dill.dump(stage, f)
                    logging.info(f"[GradientBoosting] –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç {i+1}")
                except Exception as e:
                    logging.warning(f"[GradientBoosting] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç {i+1}: {e}")
        return self

class CheckpointXGBoost(XGBClassifier):
    def __init__(self, n_estimators=100, max_depth=3, learning_rate=0.1,
                 min_child_weight=1, subsample=1.0, colsample_bytree=1.0,
                 random_state=None, objective=None, **kwargs):
        super().__init__(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            min_child_weight=min_child_weight,
            subsample=subsample,
            colsample_bytree=colsample_bytree,
            random_state=random_state,
            objective=objective,
            **kwargs
        )
        self._checkpoint_dir = get_checkpoint_path("xgboost", market_type)

    def get_params(self, deep=True):
        params = super().get_params(deep=deep)
        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–∞—à –ø—Ä–∏–≤–∞—Ç–Ω—ã–π –∞—Ç—Ä–∏–±—É—Ç, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–¥–∞–≤–∞–ª—Å—è –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –ø—Ä–∏ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏
        if '_checkpoint_dir' in params:
            del params['_checkpoint_dir']
        return params

    def fit(self, X, y, **kwargs):
        logging.info("[XGBoost] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏")
        model_path = os.path.join(self._checkpoint_dir, "xgboost_checkpoint")
        final_checkpoint = f"{model_path}_final.joblib"
        
        # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç, –ø—ã—Ç–∞–µ–º—Å—è –µ–≥–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å
        if os.path.exists(final_checkpoint):
            try:
                saved_model = joblib.load(final_checkpoint)
                if hasattr(saved_model, 'n_features_') and saved_model.n_features_ == X.shape[1]:
                    logging.info("[XGBoost] –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
                    return saved_model
            except Exception as e:
                logging.warning(f"[XGBoost] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç: {e}")
        
        # –û—á–∏—â–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞—ë–º –µ—ë –∑–∞–Ω–æ–≤–æ
        if os.path.exists(self._checkpoint_dir):
            shutil.rmtree(self._checkpoint_dir, onerror=_on_rm_error)
        ensure_directory(self._checkpoint_dir)
        
        # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–∑–≤–∞—Ç—å super().fit() —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –æ—à–∏–±–∫–∞ –∏–∑-–∑–∞ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ early_stopping_rounds, —É–¥–∞–ª—è–µ–º –µ–≥–æ –∏ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞.
        try:
            super().fit(X, y, **kwargs)
        except TypeError as e:
            if 'early_stopping_rounds' in kwargs:
                logging.warning("[XGBoost] –ü–∞—Ä–∞–º–µ—Ç—Ä early_stopping_rounds –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –ø—Ä–æ–±—É–µ–º –±–µ–∑ –Ω–µ–≥–æ")
                kwargs.pop('early_stopping_rounds')
                super().fit(X, y, **kwargs)
            else:
                raise e
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ —á–µ–∫–ø–æ–∏–Ω—Ç (–¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è)
        joblib.dump(self, final_checkpoint)
        logging.info("[XGBoost] –°–æ—Ö—Ä–∞–Ω–µ–Ω –Ω–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç")
        return self



class CheckpointLightGBM(LGBMClassifier):
    def __init__(self, n_estimators=100, num_leaves=31, learning_rate=0.1,
                 min_data_in_leaf=20, max_depth=-1, random_state=None, **kwargs):
        self._checkpoint_dir = kwargs.pop('_checkpoint_dir', get_checkpoint_path("lightgbm", market_type))
        super().__init__(n_estimators=n_estimators, num_leaves=num_leaves,
                         learning_rate=learning_rate, min_data_in_leaf=min_data_in_leaf,
                         max_depth=max_depth, random_state=random_state, **kwargs)

    def __getstate__(self):
        state = self.__dict__.copy()
        state.pop('_checkpoint_dir', None)
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._checkpoint_dir = get_checkpoint_path("lightgbm", market_type)

    def get_params(self, deep=True):
        params = super().get_params(deep=deep)
        params.pop('_checkpoint_dir', None)
        return params

    def fit(self, X, y, **kwargs):
        logging.info("[LightGBM] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏")
        model_path = os.path.join(self._checkpoint_dir, "lightgbm_checkpoint")
        final_checkpoint = f"{model_path}_final.pkl"
        if os.path.exists(final_checkpoint):
            try:
                with open(final_checkpoint, 'rb') as f:
                    saved_model = dill.load(f)
                if hasattr(saved_model, '_n_features') and saved_model._n_features == X.shape[1]:
                    logging.info("[LightGBM] –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
                    self.__dict__.update(saved_model.__dict__)
                    _ = self.predict(X[:1])
                    return self
            except Exception as e:
                logging.warning(f"[LightGBM] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç: {e}")
        if os.path.exists(self._checkpoint_dir):
            shutil.rmtree(self._checkpoint_dir, onerror=_on_rm_error)
        ensure_directory(self._checkpoint_dir)
        super().fit(X, y, **kwargs)
        self._n_features = X.shape[1]
        with open(final_checkpoint, 'wb') as f:
            dill.dump(self, f)
        logging.info("[LightGBM] –°–æ—Ö—Ä–∞–Ω–µ–Ω –Ω–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç")
        return self

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ñ–ª–∞–≥ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ (True ‚Äì —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–µ –±—É–¥–µ–º)
SKIP_CHECKPOINT = False

class CheckpointCatBoost(CatBoostClassifier):
    def __init__(self, iterations=1000, depth=6, learning_rate=0.1,
                 random_state=None, **kwargs):
        # –ï—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä 'save_snapshot' –ø–µ—Ä–µ–¥–∞–Ω, —É–¥–∞–ª—è–µ–º –µ–≥–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
        if 'save_snapshot' in kwargs:
            del kwargs['save_snapshot']
        super().__init__(iterations=iterations,
                         depth=depth,
                         learning_rate=learning_rate,
                         random_state=random_state,
                         save_snapshot=False,
                         **kwargs)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç—å –∫ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–º —Ç–æ—á–∫–∞–º –≤ –ø—Ä–∏–≤–∞—Ç–Ω–æ–º –∞—Ç—Ä–∏–±—É—Ç–µ
        self._checkpoint_dir = get_checkpoint_path("catboost", market_type)

    def get_params(self, deep=True):
        """
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º get_params, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å _checkpoint_dir –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤,
        –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –ø—Ä–∏ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, GridSearchCV).
        """
        params = super().get_params(deep=deep)
        if '_checkpoint_dir' in params:
            del params['_checkpoint_dir']
        return params

    def fit(self, X, y, **kwargs):
        logging.info("[CatBoost] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏")
        model_path = os.path.join(self._checkpoint_dir, "catboost_checkpoint")
        final_checkpoint = f"{model_path}_final.joblib"
        
        # –ï—Å–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—ã—Ç–∞–µ–º—Å—è –µ–≥–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å
        if os.path.exists(final_checkpoint):
            try:
                saved_model = joblib.load(final_checkpoint)
                if hasattr(saved_model, 'feature_count_') and saved_model.feature_count_ == X.shape[1]:
                    logging.info("[CatBoost] –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
                    return saved_model
            except Exception as e:
                logging.warning(f"[CatBoost] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: {e}")
        
        # –£–¥–∞–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏ —Å–æ–∑–¥–∞–µ–º –∑–∞–Ω–æ–≤–æ
        if os.path.exists(self._checkpoint_dir):
            shutil.rmtree(self._checkpoint_dir, onerror=_on_rm_error)
        ensure_directory(self._checkpoint_dir)
        
        # –£–¥–∞–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä save_snapshot –∏–∑ kwargs, –µ—Å–ª–∏ –æ–Ω –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        if 'save_snapshot' in kwargs:
            del kwargs['save_snapshot']
        
        super().fit(X, y, **kwargs)
        
        # –ï—Å–ª–∏ –º—ã –Ω–µ –≤ —Ä–µ–∂–∏–º–µ GridSearch (–≥–¥–µ SKIP_CHECKPOINT=True), —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç
        if not SKIP_CHECKPOINT:
            joblib.dump(self, final_checkpoint)
            logging.info("[CatBoost] –°–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç")
        else:
            logging.info("[CatBoost] –ü—Ä–æ–ø—É—Å–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (—Ä–µ–∂–∏–º GridSearch)")
        
        return self


class CheckpointRandomForest(RandomForestClassifier):
    def __init__(self, n_estimators=100, max_depth=None,
                 min_samples_split=2, min_samples_leaf=1, random_state=None, **kwargs):
        self._checkpoint_dir = kwargs.pop('_checkpoint_dir', get_checkpoint_path("random_forest", market_type))
        super().__init__(n_estimators=n_estimators, max_depth=max_depth,
                         min_samples_split=min_samples_split,
                         min_samples_leaf=min_samples_leaf, random_state=random_state, **kwargs)

    def __getstate__(self):
        state = self.__dict__.copy()
        state.pop('_checkpoint_dir', None)
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._checkpoint_dir = get_checkpoint_path("random_forest", market_type)

    def get_params(self, deep=True):
        params = super().get_params(deep=deep)
        params.pop('_checkpoint_dir', None)
        return params

    def fit(self, X, y):
        logging.info("[RandomForest] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏")
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        n_features = X.shape[1]
        existing_trees = []
        for i in range(self.n_estimators):
            cp_path = os.path.join(self._checkpoint_dir, f"random_forest_tree_{i+1}.pkl")
            if os.path.exists(cp_path):
                try:
                    with open(cp_path, 'rb') as f:
                        tree = dill.load(f)
                    if hasattr(tree, 'tree_') and tree.tree_.n_features == n_features:
                        existing_trees.append(tree)
                        logging.info(f"[RandomForest] –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–µ—Ä–µ–≤–æ {i+1} –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
                except Exception as e:
                    logging.warning(f"[RandomForest] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç {i+1}: {e}")
        if not existing_trees:
            if os.path.exists(self._checkpoint_dir):
                shutil.rmtree(self._checkpoint_dir, onerror=_on_rm_error)
            ensure_directory(self._checkpoint_dir)
            logging.info("[RandomForest] –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")
            super().fit(X, y)
        else:
            self.estimators_ = existing_trees
            remaining = self.n_estimators - len(existing_trees)
            if remaining > 0:
                logging.info(f"[RandomForest] –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ: –æ—Å—Ç–∞–ª–æ—Å—å {remaining} –¥–µ—Ä–µ–≤—å–µ–≤")
                orig_n_classes = self.n_classes_
                self.n_estimators = remaining
                super().fit(X, y)
                self.n_classes_ = orig_n_classes
                self.estimators_.extend(existing_trees)
                self.n_estimators = len(self.estimators_)
        for i, estimator in enumerate(self.estimators_):
            cp_path = os.path.join(self._checkpoint_dir, f"random_forest_tree_{i+1}.pkl")
            if not os.path.exists(cp_path):
                try:
                    with open(cp_path, 'wb') as f:
                        dill.dump(estimator, f)
                    logging.info(f"[RandomForest] –°–æ–∑–¥–∞–Ω —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –¥–µ—Ä–µ–≤–∞ {i+1}")
                except Exception as e:
                    logging.warning(f"[RandomForest] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –¥–µ—Ä–µ–≤–∞ {i+1}: {e}")
        if not hasattr(self, 'n_outputs_'):
            self.n_outputs_ = 1 if y.ndim == 1 else y.shape[1]
        return self
    
        
def save_ensemble_checkpoint(ensemble_model, checkpoint_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—â–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –∞–Ω—Å–∞–º–±–ª—è."""
    ensure_directory(os.path.dirname(checkpoint_path))
    joblib.dump(ensemble_model, checkpoint_path)
    logging.info(f"[Ensemble] –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç –∞–Ω—Å–∞–º–±–ª—è: {checkpoint_path}")



def load_ensemble_checkpoint(checkpoint_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—â–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –∞–Ω—Å–∞–º–±–ª—è."""
    if os.path.exists(checkpoint_path):
        logging.info(f"[Ensemble] –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –∞–Ω—Å–∞–º–±–ª—è: {checkpoint_path}")
        return joblib.load(checkpoint_path)
    logging.info(f"[Ensemble] –ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {checkpoint_path}")
    return None


# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
def get_historical_data(symbols, bearish_periods, interval="1m", save_path="binance_data_bearish.csv"):
    base_url_monthly = "https://data.binance.vision/data/spot/monthly/klines"
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    all_data = []
    downloaded_files = set()
    download_lock = Lock()  # –î–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞

    def download_and_process(symbol, period):
        start_date = datetime.strptime(period["start"], "%Y-%m-%d")
        end_date = datetime.strptime(period["end"], "%Y-%m-%d")
        temp_data = []

        for current_date in pd.date_range(start=start_date, end=end_date, freq='MS'):
            year = current_date.year
            month = current_date.month
            month_str = f"{month:02d}"
            file_name = f"{symbol}-{interval}-{year}-{month_str}.zip"
            file_url = f"{base_url_monthly}/{symbol}/{interval}/{file_name}"

            with download_lock:
                if file_name in downloaded_files:
                    logging.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {file_name}, —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
                    continue

            # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ (–¥–æ 3-—Ö)
            success = False
            for attempt in range(3):
                try:
                    logging.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞: {file_url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1})")
                    head_resp = requests.head(file_url, timeout=10)
                    if head_resp.status_code != 200:
                        logging.warning(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_url}")
                        break
                    logging.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {file_url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1})...")
                    response = requests.get(file_url, timeout=15)
                    if response.status_code != 200:
                        logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_url}: –ö–æ–¥ {response.status_code}")
                        continue
                    # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ, —Ñ–∏–∫—Å–∏—Ä—É–µ–º —Ñ–∞–π–ª –≤ –∫—ç—à–µ –∏ –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞
                    with download_lock:
                        downloaded_files.add(file_name)
                    success = True
                    break
                except Exception as e:
                    logging.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt+1} –¥–ª—è {file_url} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                    time.sleep(2)  # –Ω–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
            if not success:
                logging.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {file_url} –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫.")
                continue

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫–∞—á–∞–Ω–Ω–æ–≥–æ zip-—Ñ–∞–π–ª–∞
            try:
                zip_file = zipfile.ZipFile(BytesIO(response.content))
                csv_file = file_name.replace('.zip', '.csv')
                with zip_file.open(csv_file) as file:
                    df = pd.read_csv(
                        file, header=None, 
                        names=["timestamp", "open", "high", "low", "close", "volume",
                               "close_time", "quote_asset_volume", "number_of_trades",
                               "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"],
                        dtype={
                            "timestamp": "int64",
                            "open": "float32",
                            "high": "float32",
                            "low": "float32",
                            "close": "float32",
                            "volume": "float32",
                            "quote_asset_volume": "float32",
                            "number_of_trades": "int32",
                            "taker_buy_base_asset_volume": "float32",
                            "taker_buy_quote_asset_volume": "float32"
                        }
                    )
                    if "timestamp" not in df.columns:
                        logging.error(f"‚ùå –û—à–∏–±–∫–∞: –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ df –¥–ª—è {symbol}")
                        continue
                    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", errors="coerce")
                    df.set_index("timestamp", inplace=True)
                    df["symbol"] = symbol
                    temp_data.append(df)
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {symbol} –∑–∞ {current_date.strftime('%Y-%m')}: {e}")
            time.sleep(0.5)
        return pd.concat(temp_data) if temp_data else None

    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(download_and_process, symbol, period)
                   for symbol in symbols for period in bearish_periods]
        for future in futures:
            result = future.result()
            if result is not None:
                all_data.append(result)
    
    if not all_data:
        logging.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –¥–∞–Ω–Ω—ã—Ö.")
        return None
    
    df = pd.concat(all_data, ignore_index=False)
    logging.info(f"üìä –ö–æ–ª–æ–Ω–∫–∏ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º df: {df.columns}")
    
    if "timestamp" not in df.columns and not isinstance(df.index, pd.DatetimeIndex):
        logging.error(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {df.columns}")
        return None

    df = df.resample('1min').ffill()
    num_nans = df.isna().sum().sum()
    if num_nans > 0:
        nan_percentage = num_nans / len(df)
        if nan_percentage > 0.05:
            logging.warning(f"‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ {nan_percentage:.2%} –¥–∞–Ω–Ω—ã—Ö! –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN.")
            df.dropna(inplace=True)
        else:
            logging.info(f"üîß –ó–∞–ø–æ–ª–Ω—è–µ–º {nan_percentage:.2%} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–º ffill.")
            df.fillna(method='ffill', inplace=True)
    df.to_csv(save_path)
    logging.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")
    return save_path


def load_bearish_data(symbols, bearish_periods, interval="1m", save_path="binance_data_bearish.csv"):
    if os.path.exists(save_path):
        try:
            existing_data = pd.read_csv(save_path, index_col=0, parse_dates=True, on_bad_lines='skip')
            logging.info(f"–°—á–∏—Ç–∞–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {save_path}, —Å—Ç—Ä–æ–∫: {len(existing_data)}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {save_path}: {e}")
            existing_data = pd.DataFrame()
    else:
        existing_data = pd.DataFrame()
    
    all_data = {}
    logging.info(f"üöÄ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –∑–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(get_historical_data, [symbol], bearish_periods, interval, save_path): symbol
                   for symbol in symbols}
        for future in futures:
            symbol = futures[future]
            try:
                temp_file_path = future.result()
                if temp_file_path is not None:
                    new_data = pd.read_csv(temp_file_path, index_col=0, parse_dates=True, on_bad_lines='skip')
                    if symbol in all_data:
                        all_data[symbol].append(new_data)
                    else:
                        all_data[symbol] = [new_data]
                    logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–ª—è {symbol}. –§–∞–π–ª–æ–≤: {len(all_data[symbol])}")
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
    
    for symbol in list(all_data.keys()):
        if all_data[symbol]:
            all_data[symbol] = pd.concat(all_data[symbol])
        else:
            del all_data[symbol]
    
    if all_data:
        new_combined = pd.concat(all_data.values(), ignore_index=False)
    else:
        new_combined = pd.DataFrame()
    
    if not existing_data.empty:
        combined = pd.concat([existing_data, new_combined], ignore_index=False)
    else:
        combined = new_combined
    
    combined.to_csv(save_path)
    logging.info(f"üíæ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path} (–∏—Ç–æ–≥–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫: {len(combined)})")
    return all_data


def load_bearish_data(symbols, bearish_periods, interval="1m", save_path="binance_data_bearish.csv"):
    if os.path.exists(save_path):
        try:
            existing_data = pd.read_csv(save_path, index_col=0, parse_dates=True, on_bad_lines='skip')
            logging.info(f"–°—á–∏—Ç–∞–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {save_path}, —Å—Ç—Ä–æ–∫: {len(existing_data)}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {save_path}: {e}")
            existing_data = pd.DataFrame()
    else:
        existing_data = pd.DataFrame()

    all_data = {}
    logging.info(f"üöÄ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –∑–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")

    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(get_historical_data, [symbol], bearish_periods, interval, save_path): symbol
                   for symbol in symbols}
        for future in futures:
            symbol = futures[future]
            try:
                temp_file_path = future.result()
                if temp_file_path is not None:
                    new_data = pd.read_csv(temp_file_path, index_col=0, parse_dates=True, on_bad_lines='skip')
                    if symbol in all_data:
                        all_data[symbol].append(new_data)
                    else:
                        all_data[symbol] = [new_data]
                    logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–ª—è {symbol}. –§–∞–π–ª–æ–≤: {len(all_data[symbol])}")
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")

    # –ï—Å–ª–∏ –ø–æ –∫–∞–∫–∏–º-—Ç–æ —Å–∏–º–≤–æ–ª–∞–º –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ–∫–∞–∑–∞–ª–æ—Å—å, –≤—ã–≤–æ–¥–∏–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –Ω–æ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
    if not all_data:
        logging.warning("–ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ.")
        return existing_data if not existing_data.empty else {}

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    for symbol in list(all_data.keys()):
        if all_data[symbol]:
            all_data[symbol] = pd.concat(all_data[symbol])
        else:
            del all_data[symbol]

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–¥–∏–Ω DataFrame
    new_combined = pd.concat(all_data.values(), ignore_index=False) if all_data else pd.DataFrame()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    combined = pd.concat([existing_data, new_combined], ignore_index=False) if not existing_data.empty else new_combined

    combined.to_csv(save_path)
    logging.info(f"üíæ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path} (–∏—Ç–æ–≥–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫: {len(combined)})")
    return all_data


    
'''def aggregate_to_2min(data):
    """
    –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ 1 –º–∏–Ω—É—Ç–∞ –¥–æ 2 –º–∏–Ω—É—Ç.
    """
    logging.info("–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ 1 –º–∏–Ω—É—Ç–∞ –¥–æ 2 –º–∏–Ω—É—Ç")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å
    if not isinstance(data.index, pd.DatetimeIndex):
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        if 'timestamp' in data.columns:
            data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ datetime
            data.set_index('timestamp', inplace=True)
        else:
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")

        # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏–Ω–¥–µ–∫—Å —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex
        if not isinstance(data.index, pd.DatetimeIndex):
            raise ValueError("–ò–Ω–¥–µ–∫—Å –¥–∞–Ω–Ω—ã—Ö –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex –¥–∞–∂–µ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.")

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    data = data.resample('2T').agg({
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum'
    }).dropna()

    logging.info(f"–ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(data)} —Å—Ç—Ä–æ–∫")
    logging.info(f"–ü–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ 2 –º–∏–Ω—É—Ç—ã: NaN = {data.isna().sum().sum()}")
    return data'''


# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def extract_features(data):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞.
    –ü–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É.
    """
    logging.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞")
    data = data.copy()
    
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    for col in ['open', 'high', 'low', 'close', 'volume']:
        data[col] = pd.to_numeric(data[col], errors='coerce')
    
    # 1. –ë–∞–∑–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è: –≤–æ–∑–≤—Ä–∞—Ç—ã, –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –æ–±—ä–µ–º–∞, —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Ü–µ–Ω—ã
    returns = data['close'].pct_change(fill_method=None)
    volume_ratio = data['volume'] / data['volume'].rolling(10).mean()
    price_acceleration = returns.diff()
    
    # 2. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
    def calculate_dynamic_thresholds(window=10):
        volatility = returns.rolling(window).std()
        avg_volatility = volatility.rolling(100).mean()
        volatility_ratio = volatility / avg_volatility
        base_strong = -0.001  # ~ -0.1%
        base_medium = -0.0005 # ~ -0.05%
        strong_threshold = base_strong * np.where(
            volatility_ratio > 1.5, 1.5,
            np.where(volatility_ratio < 0.5, 0.5, volatility_ratio)
        )
        medium_threshold = base_medium * np.where(
            volatility_ratio > 1.5, 1.5,
            np.where(volatility_ratio < 0.5, 0.5, volatility_ratio)
        )
        return strong_threshold, medium_threshold

    strong_threshold, medium_threshold = calculate_dynamic_thresholds()
    
    # –ü–æ—Ä–æ–≥ –¥–ª—è "BUY"
    pos_threshold = 0.0005
    
    future_return = returns.shift(-1)
    
    # 3. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (2=BUY, 1=SELL, 0=HOLD)
    data['target'] = np.where(
        (future_return > pos_threshold),
        2,
        np.where(
            (
                ((future_return < strong_threshold) & 
                 (volume_ratio > 1.2) & 
                 (price_acceleration < 0) &
                 (data['volume'] > data['volume'].rolling(20).mean()))
            ) |
            (
                (future_return < medium_threshold) & 
                (volume_ratio > 1) &
                (price_acceleration < 0)
            ),
            1,
            0
        )
    )
    
    # 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    data['returns'] = returns
    data['log_returns'] = np.log(data['close'] / data['close'].shift(1))
    
    data['volume_ma'] = data['volume'].rolling(10).mean()
    data['volume_ratio'] = data['volume'] / data['volume_ma']
    data['selling_pressure'] = data['volume'] * (data['close'] - data['open']).abs() * np.where(data['close'] < data['open'], 1, 0)
    data['buying_pressure'] = data['volume'] * (data['close'] - data['open']).abs() * np.where(data['close'] > data['open'], 1, 0)
    data['pressure_ratio'] = data['selling_pressure'] / data['buying_pressure'].replace(0, 1)
    
    data['volatility'] = returns.rolling(10).std()
    data['volatility_ma'] = data['volatility'].rolling(20).mean()
    data['volatility_ratio'] = data['volatility'] / data['volatility_ma']
    
    # 5. –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    for period in [3, 5, 8, 13, 21]:
        data[f'sma_{period}'] = SMAIndicator(data['close'], window=period).sma_indicator()
        data[f'ema_{period}'] = data['close'].ewm(span=period, adjust=False).mean()
    
    # 6. MACD
    macd = MACD(data['close'], window_slow=26, window_fast=12, window_sign=9)
    data['macd'] = macd.macd()
    data['macd_signal'] = macd.macd_signal()
    data['macd_diff'] = data['macd'] - data['macd_signal']
    data['macd_slope'] = data['macd_diff'].diff()
    
    # 7. –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    data['obv'] = OnBalanceVolumeIndicator(data['close'], data['volume']).on_balance_volume()
    data['cmf'] = ChaikinMoneyFlowIndicator(data['high'], data['low'], data['close'], data['volume']).chaikin_money_flow()
    data['volume_change'] = data['volume'].pct_change(fill_method=None)
    data['volume_ma_ratio'] = data['volume'] / data['volume'].rolling(20).mean()
    
    # 8. –û—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã
    for period in [7, 14, 21]:
        data[f'rsi_{period}'] = RSIIndicator(data['close'], window=period).rsi()
    data['stoch_k'] = StochasticOscillator(data['high'], data['low'], data['close'], window=7).stoch()
    data['stoch_d'] = StochasticOscillator(data['high'], data['low'], data['close'], window=7).stoch_signal()
    
    # 9. –£—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
    data['support_level'] = data['low'].rolling(20).min()
    data['resistance_level'] = data['high'].rolling(20).max()
    data['price_to_support'] = data['close'] / data['support_level']
    
    # 10. –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π
    data['candle_body'] = (data['close'] - data['open']).abs()
    data['upper_shadow'] = data['high'] - np.maximum(data['close'], data['open'])
    data['lower_shadow'] = np.minimum(data['close'], data['open']) - data['low']
    data['body_to_shadow_ratio'] = data['candle_body'] / (data['upper_shadow'] + data['lower_shadow']).replace(0, 0.001)
    
    # 11. –ü—Ä–æ—Ä—ã–≤—ã —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π
    data['price_level_breach'] = np.where(
        data['close'] < data['support_level'].shift(1), -1,
        np.where(data['close'] > data['resistance_level'].shift(1), 1, 0)
    )
    
    # 12. –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–∫–æ—Ä–æ—Å—Ç–∏
    data['price_acceleration'] = returns.diff()
    data['volume_acceleration'] = data['volume_change'].diff()
    
    # 13. Bollinger Bands
    bb = BollingerBands(data['close'], window=20)
    data['bb_high'] = bb.bollinger_hband()
    data['bb_low'] = bb.bollinger_lband()
    data['bb_width'] = bb.bollinger_wband()
    data['bb_position'] = (data['close'] - data['bb_low']) / (data['bb_high'] - data['bb_low'])
    
    # 14. ATR
    for period in [5, 10, 20]:
        data[f'atr_{period}'] = AverageTrueRange(data['high'], data['low'], data['close'], window=period).average_true_range()
    
    # 15. –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏
    data['micro_trend'] = np.where(data['close'] > data['close'].shift(1), 1,
                                   np.where(data['close'] < data['close'].shift(1), -1, 0))
    data['micro_trend_sum'] = data['micro_trend'].rolling(5).sum()
    data['volume_acceleration_5m'] = (data['volume'].diff() / data['volume'].rolling(5).mean()).fillna(0)
    
    # 16. –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–∏–ª—ã –º–µ–¥–≤–µ–∂—å–µ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è
    data['bearish_strength'] = np.where(
        (data['close'] < data['open']) & 
        (data['volume'] > data['volume'].rolling(20).mean()) & 
        (data['close'] == data['low']),
        3,
        np.where(
            (data['close'] < data['open']) & 
            (data['volume'] > data['volume'].rolling(20).mean()),
            2,
            np.where(data['close'] < data['open'], 1, 0)
        )
    )
    
    # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É, —Ç–∞–∫ –∫–∞–∫ future_return –¥–ª—è –Ω–µ—ë –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω
    data = data[:-1]
    
    return data.replace([np.inf, -np.inf], np.nan).ffill().bfill()



def clean_data(X, y):
    """
    –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤.
    """
    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    mask = X.notnull().all(axis=1)
    X_clean = X.loc[mask]
    y_clean = y.loc[mask]  # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤
    logging.debug(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤: X = {X_clean.shape}, y = {y_clean.shape}")

    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ X
    duplicated_indices = X_clean.index.duplicated(keep='first')
    X_clean = X_clean.loc[~duplicated_indices]
    y_clean = y_clean.loc[~duplicated_indices]  # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤
    logging.debug(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: X = {X_clean.shape}, y = {y_clean.shape}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤
    if not X_clean.index.equals(y_clean.index):
        raise ValueError("–ò–Ω–¥–µ–∫—Å—ã X –∏ y –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")

    return X_clean, y_clean


# –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
def remove_outliers(data):
    numeric_data = data.select_dtypes(include=[np.number])  # –¢–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    Q1 = numeric_data.quantile(0.25)
    Q3 = numeric_data.quantile(0.75)
    IQR = Q3 - Q1
    filtered_data = data[~((numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))).any(axis=1)]
    logging.debug(f"–ò–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤: {filtered_data.index}")
    return filtered_data


# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
def add_clustering_feature(data):
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞
    """
    features_for_clustering = [
        # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        'close', 'volume', 'returns', 'log_returns',
        
        # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –µ—Å—Ç—å
        'volume_ratio', 'volume_ma', 'cmf', 'obv',
        
        # –û—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–∑–¥–∞—é—Ç—Å—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        'rsi_7', 'rsi_14', 'rsi_21',  # RSI –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        'stoch_k', 'stoch_d',
        
        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –≤ extract_features
        'macd', 'macd_signal', 'macd_diff',
        'sma_3', 'sma_5', 'sma_8',    # –ö–æ—Ä–æ—Ç–∫–∏–µ SMA
        'ema_3', 'ema_5', 'ema_8',    # –ö–æ—Ä–æ—Ç–∫–∏–µ EMA
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        'bb_width', 'atr_5', 'atr_10',
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞
        'selling_pressure', 'pressure_ratio',
        'price_acceleration', 'volume_acceleration'
    ]

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    available_features = [f for f in features_for_clustering if f in data.columns]
    
    if len(available_features) < 5:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    kmeans = KMeans(n_clusters=5, random_state=42)
    data['cluster'] = kmeans.fit_predict(data[available_features])
    
    logging.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {len(available_features)}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:\n{data['cluster'].value_counts()}")
    
    return data


# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞)
def augment_data(data):
    noise = np.random.normal(0, 0.01, data.shape)
    augmented_data = data + noise
    augmented_data.index = data.index  # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
    logging.debug(f"–ò–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ augment_data: {augmented_data.index}")
    return augmented_data

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è SMOTETomek
def smote_process(X_chunk, y_chunk, chunk_id):
    smote_tomek = SMOTETomek(random_state=42)
    X_resampled, y_resampled = smote_tomek.fit_resample(X_chunk, y_chunk)
    return X_resampled, y_resampled

def parallel_smote(X, y, n_chunks=4):
    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –∏—Å—Ö–æ–¥–Ω—ã–π –º–∞—Å—Å–∏–≤ X —è–≤–ª—è–µ—Ç—Å—è –∫–æ–ø–∏–µ–π (–∏–∑–º–µ–Ω—è–µ–º–æ–π)
    X = np.copy(X)
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–ª–∞–µ–º –∫–æ–ø–∏—é
    X_chunks = [np.copy(chunk) for chunk in np.array_split(X, n_chunks)]
    y_chunks = np.array_split(y, n_chunks)
    results = Parallel(n_jobs=n_chunks)(
        delayed(smote_process)(X_chunk, y_chunk, idx)
        for idx, (X_chunk, y_chunk) in enumerate(zip(X_chunks, y_chunks))
    )
    X_resampled = np.vstack([res[0] for res in results])
    y_resampled = np.hstack([res[1] for res in results])
    return X_resampled, y_resampled


# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
def prepare_data(data):
    logging.info("–ù–∞—á–∞–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")

    # –ï—Å–ª–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–æ–≤–∞—Ä—ë–º (–ø–æ –º–æ–Ω–µ—Ç–∞–º), –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö –≤ –æ–¥–∏–Ω DataFrame
    if isinstance(data, dict):
        if not data:
            raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã (–ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å)")
        data = pd.concat(data.values())
    
    if data.empty:
        raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã")
    logging.info(f"–ò—Å—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {data.shape}")

    # –ü—Ä–∏–≤–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –∫ DatetimeIndex (–µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —Ç–∞–∫)
    if not isinstance(data.index, pd.DatetimeIndex):
        try:
            dt_index = pd.to_datetime(data.index, errors='coerce')
            if dt_index.isnull().all():
                raise ValueError("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å –≤ DatetimeIndex")
            data.index = dt_index
            # –ï—Å–ª–∏ —Å—Ç–æ–ª–±—Ü–∞ 'timestamp' –Ω–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
            if 'timestamp' not in data.columns:
                data['timestamp'] = dt_index
            logging.info("–ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ DatetimeIndex –∏, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –¥–æ–±–∞–≤–ª–µ–Ω –∫–∞–∫ –∫–æ–ª–æ–Ω–∫–∞ 'timestamp'.")
        except Exception as e:
            raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ 'timestamp'.") from e

    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: –∞–Ω–æ–º–∞–ª–∏–∏, –æ–±—ä–µ–º, —à—É–º –∏ —Ç.–¥.
    data = detect_anomalies(data)
    logging.info("–ê–Ω–æ–º–∞–ª–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏ –ø–æ–º–µ—á–µ–Ω—ã")
    
    data = validate_volume_confirmation(data)
    logging.info("–î–æ–±–∞–≤–ª–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –æ–±—ä–µ–º–æ–º")
    
    data = remove_noise(data)
    logging.info("–®—É–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω")
    
    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –º–æ–Ω–µ—Ç–∞–º ‚Äì –¥–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–ª–∏ —Å–ª–æ–≤–∞—Ä—å)
    if isinstance(data, dict):
        data = calculate_cross_coin_features(data)
        logging.info("–î–æ–±–∞–≤–ª–µ–Ω—ã –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
    
    data = extract_features(data)
    logging.info(f"–ü–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {data.shape}")

    # –¢–µ–ø–µ—Ä—å –æ—Ç–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ç —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º –Ω–µ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'symbol', 'close_time', 'ignore')
    drop_cols = ['symbol', 'close_time', 'ignore']
    X = data.drop(columns=drop_cols + ['target'], errors='ignore')
    y = data['target']
    
    # –í–∞–∂–Ω–æ: –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—É–±–∏—Ä–∞–µ–º datetime, –µ—Å–ª–∏ –æ–Ω –æ—Å—Ç–∞–ª—Å—è)
    X = X.select_dtypes(include=[np.number])
    
    X, y = clean_data(X, y)
    logging.info(f"–ü–æ—Å–ª–µ clean_data: X = {X.shape}, y = {y.shape}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTETomek ‚Äì —Ç–µ–ø–µ—Ä—å X —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    X_resampled, y_resampled = parallel_smote(X, y, n_chunks=4)
    logging.info(f"–ü–æ—Å–ª–µ SMOTETomek: X = {X_resampled.shape}, y = {len(y_resampled)}")
    
    # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –ø–µ—Ä–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É target
    resampled_data = pd.DataFrame(X_resampled, columns=X.columns)
    resampled_data['target'] = y_resampled
    
    resampled_data = remove_outliers(resampled_data)
    logging.info(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤: {resampled_data.shape}")
    
    resampled_data = add_clustering_feature(resampled_data)
    logging.info(f"–ü–æ—Å–ª–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {resampled_data.shape}")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã, –∏—Å–∫–ª—é—á–∞—è 'target'
    numeric_cols = resampled_data.select_dtypes(include=[np.number]).columns.tolist()
    features = [col for col in numeric_cols if col != 'target']
    logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target:\n{resampled_data['target'].value_counts()}")
    
    return resampled_data, features


def get_checkpoint_path(model_name, market_type):
    checkpoint_path = os.path.join("checkpoints", market_type, model_name)
    ensure_directory(checkpoint_path)
    return checkpoint_path

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤
def balance_classes(X, y):
    """
    –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SMOTETomek.
    """
    smt = SMOTETomek(random_state=42)
    X_res, y_res = smt.fit_resample(X, y)
    return X_res, y_res


def check_class_balance(y):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤"""
    class_counts = pd.Series(y).value_counts()
    total = len(y)
    
    logging.info("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    for class_label, count in class_counts.items():
        percentage = (count / total) * 100
        logging.info(f"–ö–ª–∞—Å—Å {class_label}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.2f}%)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å
    if class_counts.max() / class_counts.min() > 10:
        logging.warning("–°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ (>10:1)")
        

def check_feature_quality(X, y):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏—Å–ø–µ—Ä—Å–∏—é
    zero_var_features = []
    for col in X.columns:
        if X[col].std() == 0:
            zero_var_features.append(col)
    if zero_var_features:
        logging.warning(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π: {zero_var_features}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    corr_matrix = X.corr()
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i,j]) > 0.95:
                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j]))
    if high_corr_pairs:
        logging.warning(f"–°–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {high_corr_pairs}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    selector = SelectKBest(score_func=f_classif, k='all')
    selector.fit(X, y)
    feature_scores = pd.DataFrame({
        'Feature': X.columns,
        'Score': selector.scores_
    }).sort_values('Score', ascending=False)
    logging.info("–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    logging.info(feature_scores.head(10))



# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def train_model(model, X_train, y_train, name):
    logging.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {name}")
    model.fit(X_train, y_train)
    return model

def load_progress(base_learners, meta_model, checkpoint_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫.
    """
    for i, (name, model) in enumerate(base_learners):
        intermediate_path = f"{checkpoint_path}_{name}.pkl"
        if os.path.exists(intermediate_path):
            logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –º–æ–¥–µ–ª–∏ {name} –∏–∑ {intermediate_path}")
            base_learners[i] = (name, joblib.load(intermediate_path))
        else:
            logging.info(f"–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –¥–ª—è {name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è.")
    
    meta_model_path = f"{checkpoint_path}_meta.pkl"
    if not os.path.exists(os.path.dirname(meta_model_path)):
        os.makedirs(os.path.dirname(meta_model_path))
    if os.path.exists(meta_model_path):
        logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏–∑ {meta_model_path}")
        meta_model = joblib.load(meta_model_path)
    else:
        logging.info("–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –¥–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è.")
    
    return base_learners, meta_model


# –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
def train_ensemble_model(data, selected_features, model_filename='models/bearish_stacked_ensemble_model.pkl'):
    """
    –û–±—É—á–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞ (3 –∫–ª–∞—Å—Å–∞: hold=0, sell=1, buy=2).

    –®–∞–≥–∏:
      1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
      2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ DatetimeIndex –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–ª–æ–Ω–∫–∏ 'timestamp'.
      3. –í—ã–±–æ—Ä —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
      4. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤, —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è.
      5. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —á–µ—Ä–µ–∑ SMOTETomek –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ –¥–ª—è GridSearchCV.
      6. –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GridSearchCV.
      7. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ–ª–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤.
      8. –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ç–µ–∫–∏–Ω–≥-–∞–Ω—Å–∞–º–±–ª—è.
      9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫–∞.

    :param data: DataFrame —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'target').
    :param selected_features: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–Ω–µ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'target' –∏ 'timestamp').
    :param model_filename: –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è.
    :return: —Å–ª–æ–≤–∞—Ä—å —Å –æ–±—É—á–µ–Ω–Ω—ã–º –∞–Ω—Å–∞–º–±–ª–µ–º, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫–æ–º –∏ —Å–ø–∏—Å–∫–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    """
    
    logging.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞ (3 –∫–ª–∞—Å—Å–∞: hold/sell/buy)")
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if data.empty:
        raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã")
    if not isinstance(selected_features, list):
        raise TypeError("selected_features –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
    if 'target' not in data.columns:
        raise KeyError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'target' –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    # –£–¥–∞–ª—è–µ–º —Å–ª—É—á–∞–π–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π 'timestamp' –∏–∑ —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    selected_features = [feat for feat in selected_features if feat != 'timestamp']
    
    logging.info(f"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ shape: {data.shape}")
    logging.info(f"–í—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {data.columns.tolist()}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–æ –æ–±—É—á–µ–Ω–∏—è:\n{data['target'].value_counts()}")
    
    # 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ DatetimeIndex
    if not isinstance(data.index, pd.DatetimeIndex):
        data.index = pd.to_datetime(data.index, errors='coerce')
        if data.index.isnull().all():
            raise ValueError("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å –≤ DatetimeIndex.")
        logging.info("–ò–Ω–¥–µ–∫—Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ DatetimeIndex.")
    
    # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–ª–æ–Ω–∫–∏ 'timestamp'
    if 'timestamp' in data.columns:
        logging.info("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' —É–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —É–¥–∞–ª—è–µ–º –µ—ë –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è.")
        data = data.drop(columns=['timestamp'])
    data['timestamp'] = data.index.copy()
    data.reset_index(drop=True, inplace=True)
    data = data.loc[:, ~data.columns.duplicated()]
    logging.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–æ–ª–±—Ü–∞ 'timestamp' –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    
    # 4. –í—ã–±–æ—Ä —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    y = data['target'].copy()
    X = data[selected_features].copy()
    logging.info(f"–†–∞–∑–º–µ—Ä X –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {X.shape}")
    logging.info(f"–†–∞–∑–º–µ—Ä y –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {y.shape}")
    logging.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è y: {np.unique(y, return_counts=True)}")
    
    X = X.astype(float)
    y = y.astype(int)
    mask = ~np.isnan(y)
    X = X[mask]
    y = y[mask]
    if X.size == 0 or y.size == 0:
        logging.error("X –∏–ª–∏ y –ø—É—Å—Ç—ã –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö.")
        raise ValueError("X –∏–ª–∏ y –ø—É—Å—Ç—ã –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN.")
    logging.info(f"–†–∞–∑–º–µ—Ä X –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {X.shape}")
    logging.info(f"–†–∞–∑–º–µ—Ä y –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {y.shape}")
    
    # 5. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
    try:
        X_resampled, y_resampled = balance_classes(X, y)
        logging.info(f"–†–∞–∑–º–µ—Ä—ã –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X_resampled={X_resampled.shape}, y_resampled={y_resampled.shape}")
        logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:\n{pd.Series(y_resampled).value_counts()}")
    except ValueError as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–µ –∫–ª–∞—Å—Å–æ–≤: {e}")
        raise
    
    X_resampled = pd.DataFrame(X_resampled, columns=X.columns)
    
    # 6. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ (—Å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π)
    X_train, X_test, y_train, y_test = train_test_split(
        X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
    )
    logging.info(f"Train size: X_train={X_train.shape}, y_train={y_train.shape}")
    logging.info(f"Test size: X_test={X_test.shape}, y_test={y_test.shape}")
    
    # 7. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_train_aug = augment_data(X_train_scaled_df)
    logging.info(f"–ü–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: X_train_aug = {X_train_aug.shape}")
    
    # 8. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —á–µ—Ä–µ–∑ SMOTETomek
    smote_tomek = SMOTETomek(random_state=42)
    X_resampled2, y_resampled2 = smote_tomek.fit_resample(X_train_aug, y_train)
    logging.info(f"–ü–æ—Å–ª–µ SMOTETomek: X = {X_resampled2.shape}, y = {len(y_resampled2)}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ SMOTETomek:\n{pd.Series(y_resampled2).value_counts()}")
    
    # 9. –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ –¥–ª—è GridSearchCV (20% –¥–∞–Ω–Ω—ã—Ö)
    sample_size = int(0.2 * X_resampled2.shape[0])
    sample_indices = np.random.choice(np.arange(X_resampled2.shape[0]), size=sample_size, replace=False)
    if hasattr(X_resampled2, 'iloc'):
        X_sample = X_resampled2.iloc[sample_indices]
    else:
        X_sample = X_resampled2[sample_indices]
    y_sample = y_resampled2[sample_indices]
    logging.info(f"–ü–æ–¥–≤—ã–±–æ—Ä–∫–∞ –¥–ª—è GridSearchCV: X_sample = {X_sample.shape}, y_sample = {y_sample.shape}")
    
    perform_grid_search = True
    base_learners = []

    # 10. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∫–ª–∞—Å—Å–∞–º–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    rf_model = CheckpointRandomForest(n_estimators=200, max_depth=6, min_samples_leaf=5)
    gb_model = CheckpointGradientBoosting(n_estimators=150, max_depth=5, learning_rate=0.03, subsample=0.8)
    xgb_model = CheckpointXGBoost(n_estimators=150, max_depth=5, subsample=0.8,
                                  min_child_weight=3, learning_rate=0.03,
                                  objective='multi:softprob', num_class=3)
    lgbm_model = CheckpointLightGBM(n_estimators=150, num_leaves=32, learning_rate=0.03,
                                    min_data_in_leaf=5, random_state=42,
                                    objective="multiclass", num_class=3)
    catboost_model = CheckpointCatBoost(iterations=300, depth=6, learning_rate=0.03,
                                         min_data_in_leaf=5, random_state=42,
                                         loss_function='MultiClass')
    for name, model in [('rf', rf_model), ('gb', gb_model), ('xgb', xgb_model),
                        ('lgbm', lgbm_model), ('catboost', catboost_model)]:
        if perform_grid_search:
            if name == 'rf':
                param_grid = {'n_estimators': [150, 200], 'max_depth': [4, 6, 8], 'min_samples_leaf': [5, 8]}
            elif name == 'gb':
                param_grid = {'n_estimators': [150, 200], 'max_depth': [4, 5, 6],
                              'learning_rate': [0.03, 0.05], 'subsample': [0.8, 0.9]}
            elif name == 'xgb':
                param_grid = {'n_estimators': [150, 200], 'max_depth': [4, 5, 6],
                              'learning_rate': [0.03, 0.05], 'min_child_weight': [3, 5]}
            elif name == 'lgbm':
                param_grid = {'n_estimators': [150, 200], 'num_leaves': [32, 40],
                              'learning_rate': [0.03, 0.05], 'min_data_in_leaf': [5, 8]}
            elif name == 'catboost':
                param_grid = {'iterations': [300, 350], 'depth': [5, 6],
                              'learning_rate': [0.03, 0.05]}
            grid_search = GridSearchCV(model, param_grid, cv=2, scoring='f1_weighted', n_jobs=1)
            grid_search.fit(scaler.transform(X_sample), y_sample)
            logging.info(f"[GridSearch] –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è {name}: {grid_search.best_params_}")
            base_learners.append((name, grid_search.best_estimator_))
        else:
            base_learners.append((name, model))
    
    # 11. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ–ª–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    X_resampled_scaled = scaler.fit_transform(X_resampled2)
    for name, model in base_learners:
        checkpoint_path_model = get_checkpoint_path(name, "bearish")
        final_checkpoint = os.path.join(checkpoint_path_model, f"{name}_final.pkl")
        logging.info(f"[Ensemble] –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {name}")
        if isinstance(model, (CheckpointXGBoost, CheckpointLightGBM)):
            model.fit(X_resampled2, y_resampled2, eval_set=[(X_test, y_test)], early_stopping_rounds=20)
        else:
            model.fit(X_resampled_scaled, y_resampled2)
        try:
            with open(final_checkpoint, 'wb') as f:
                dill.dump(model, f, protocol=2)
            logging.info(f"[Ensemble] –ú–æ–¥–µ–ª—å {name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final_checkpoint}")
        except Exception as e:
            logging.warning(f"[Ensemble] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è {name}: {e}")
        if hasattr(model, "feature_importances_"):
            logging.info(f"[{name}] Feature importances: {model.feature_importances_}")
    
    # 12. –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ GridSearchCV

    meta_model = LogisticRegression(C=0.08, max_iter=30000, tol=1e-8, solver='saga',
                                    random_state=42, multi_class='multinomial')
    meta_param_grid = {'C': [0.01, 0.08, 0.1],
                       'penalty': ['l1', 'l2'],
                       'solver': ['saga'],
                       'max_iter': [30000],
                       'tol': [1e-8]}
    meta_grid_search = GridSearchCV(meta_model, meta_param_grid, cv=2, scoring='f1_weighted', n_jobs=1)
    meta_grid_search.fit(X_resampled_scaled, y_resampled2)
    meta_model = meta_grid_search.best_estimator_
    logging.info(f"[GridSearch] –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏: {meta_grid_search.best_params_}")
    
    # 13. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    for name, _ in base_learners:
        checkpoint_dir = get_checkpoint_path(name, "bearish")
        if os.path.exists(checkpoint_dir):
            shutil.rmtree(checkpoint_dir)
            ensure_directory(checkpoint_dir)
    
    # 14. –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ç–µ–∫–∏–Ω–≥-–∞–Ω—Å–∞–º–±–ª—è
    logging.info("[Ensemble] –û–±—É—á–µ–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥-–∞–Ω—Å–∞–º–±–ª—è (3 –∫–ª–∞—Å—Å–∞: hold/sell/buy)")
    ensemble_model = StackingClassifier(
        estimators=base_learners,
        final_estimator=meta_model,
        passthrough=True,
        cv=5,
        n_jobs=1
    )
    ensemble_model.fit(X_resampled_scaled, y_resampled2)
    
    # 15. –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    y_pred = ensemble_model.predict(scaler.transform(X_test))
    f1 = f1_score(y_test, y_pred, average='weighted')
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    logging.info(f"F1-Score (weighted): {f1:.4f}")
    logging.info(f"Precision (weighted): {precision:.4f}")
    logging.info(f"Recall (weighted): {recall:.4f}")
    
    # 16. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è
    save_data = {"ensemble_model": ensemble_model, "scaler": scaler}
    dir_path = os.path.dirname(model_filename)
    if dir_path:
        ensure_directory(dir_path)
    joblib.dump(save_data, model_filename)
    logging.info(f"[Ensemble] –ê–Ω—Å–∞–º–±–ª—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {model_filename}")
    
    return {"ensemble_model": ensemble_model, "scaler": scaler, "features": selected_features}



# –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫
if __name__ == "__main__":

    strategy = initialize_strategy()
    
    symbols = ['BTCUSDC', 'ETHUSDC', 'BNBUSDC','XRPUSDC', 'ADAUSDC', 'SOLUSDC', 'DOTUSDC', 'LINKUSDC', 'TONUSDC', 'NEARUSDC']
    
    bearish_periods = [
            {"start": "2018-01-17", "end": "2018-03-31"},
            {"start": "2018-09-01", "end": "2018-12-31"},
            {"start": "2021-05-12", "end": "2021-08-31"},
            {"start": "2022-05-01", "end": "2022-07-31"},
            {"start": "2022-09-01", "end": "2022-12-15"},
            {"start": "2022-12-16", "end": "2023-01-31"}
        ]
    
    try:
        data_dict = load_bearish_data(symbols, bearish_periods, interval="1m")
        logging.info("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        
    data, selected_features = prepare_data(data_dict)
    logging.debug(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {data.columns.tolist()}")
    logging.debug(f"–í—ã–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {selected_features}")
    
    ensemble_model, scaler, features = train_ensemble_model(data, selected_features, ensemble_model_filename)
    logging.info("–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    save_logs_to_file("–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    logging.info("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ.")
    sys.exit(0)



